<!DOCTYPE html><html lang="en" class="light" style="scroll-padding:60px"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width,initial-scale=1"/><title>Your mission - Zero or one?</title><meta property="og:title" content="Your mission - Zero or one?"/><meta name="generator" content="mystmd"/><meta name="description" content="An image classification challenge using hybrid quantum-classical neural networks"/><meta property="og:description" content="An image classification challenge using hybrid quantum-classical neural networks"/><meta name="keywords" content=""/><meta name="image" content="/qml-challenge-scorer/build/hybrid_nn-02bee137d9c820208ac883f635840f61.png"/><meta property="og:image" content="/qml-challenge-scorer/build/hybrid_nn-02bee137d9c820208ac883f635840f61.png"/><link rel="icon" href="/favicon.ico"/><link rel="stylesheet" href="/qml-challenge-scorer/build/_assets/app-R2ZFIDH4.css"/><link rel="stylesheet" href="/qml-challenge-scorer/build/_assets/thebe-core-C4ZDKLDU.css"/><link rel="stylesheet" href="/myst-theme.css"/><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/jupyter-matplotlib@0.11.3/css/mpl_widget.css"/><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.css"/><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css" integrity="sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ" crossorigin="anonymous"/></head><body class="m-0 transition-colors duration-500 bg-white dark:bg-stone-900"><div class="fixed top-1 left-1 h-[0px] w-[0px] focus-within:z-40 focus-within:h-auto focus-within:w-auto bg-white overflow-hidden focus-within:p-2 focus-within:ring-1" aria-label="skip to content options"><a href="#skip-to-frontmatter" class="block px-2 py-1 text-black underline">Skip to article frontmatter</a><a href="#skip-to-article" class="block px-2 py-1 text-black underline">Skip to article content</a></div><div class="bg-white/80 backdrop-blur dark:bg-stone-900/80 shadow dark:shadow-stone-700 p-3 md:px-8 fixed w-screen top-0 z-30 h-[60px]"><nav class="flex items-center justify-between flex-wrap max-w-[1440px] mx-auto"><div class="flex flex-row xl:min-w-[19.5rem] mr-2 sm:mr-7 justify-start items-center"><div class="block xl:hidden"><button class="flex items-center border-stone-400 text-stone-800 hover:text-stone-900 dark:text-stone-200 hover:dark:text-stone-100"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" width="2rem" height="2rem" class="m-1"><path fill-rule="evenodd" d="M3 6.75A.75.75 0 013.75 6h16.5a.75.75 0 010 1.5H3.75A.75.75 0 013 6.75zM3 12a.75.75 0 01.75-.75h16.5a.75.75 0 010 1.5H3.75A.75.75 0 013 12zm0 5.25a.75.75 0 01.75-.75h16.5a.75.75 0 010 1.5H3.75a.75.75 0 01-.75-.75z" clip-rule="evenodd"></path></svg><span class="sr-only">Open Menu</span></button></div><a class="flex items-center ml-3 dark:text-white w-fit md:ml-5 xl:ml-7" href="/qml-challenge-scorer/"><div class="p-1 mr-3"><img src="/qml-challenge-scorer/build/logo-4ab06c81ad74c531b0bcbc2fa2eb5a90.png" class="h-9 dark:hidden" height="2.25rem"/><img src="/qml-challenge-scorer/build/logo_dark-70575cd31030c6b7d14d0524bad7aeaf.png" class="hidden h-9 dark:block" height="2.25rem"/></div><span class="text-md sm:text-xl tracking-tight sm:mr-5 sr-only">Made with MyST</span></a></div><div class="flex items-center flex-grow w-auto"><div class="flex-grow hidden text-md lg:block"></div><div class="flex-grow block"></div><button class="theme rounded-full border border-stone-700 dark:border-white hover:bg-neutral-100 border-solid overflow-hidden text-stone-700 dark:text-white hover:text-stone-500 dark:hover:text-neutral-800 w-8 h-8 mx-3" title="Change theme to dark mode." aria-label="Change theme to dark mode."><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" class="h-full w-full p-0.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 3v2.25m6.364.386l-1.591 1.591M21 12h-2.25m-.386 6.364l-1.591-1.591M12 18.75V21m-4.773-4.227l-1.591 1.591M5.25 12H3m4.227-4.773L5.636 5.636M15.75 12a3.75 3.75 0 11-7.5 0 3.75 3.75 0 017.5 0z"></path></svg></button><div class="block sm:hidden"><div class="relative" data-headlessui-state=""><div><button class="flex text-sm bg-transparent rounded-full focus:outline-none" id="headlessui-menu-button-:Rciop:" type="button" aria-haspopup="menu" aria-expanded="false" data-headlessui-state=""><span class="sr-only">Open Menu</span><div class="flex items-center text-stone-200 hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" width="2rem" height="2rem" class="p-1"><path fill-rule="evenodd" d="M10.5 6a1.5 1.5 0 113 0 1.5 1.5 0 01-3 0zm0 6a1.5 1.5 0 113 0 1.5 1.5 0 01-3 0zm0 6a1.5 1.5 0 113 0 1.5 1.5 0 01-3 0z" clip-rule="evenodd"></path></svg></div></button></div></div></div><div class="hidden sm:block"><a href="https://willieab.github.io/qml-challenge-scorer/ionqvision-docs" target="_blank" rel="noopener noreferrer" class="inline-block px-4 py-2 mx-1 mt-0 leading-none border rounded text-md border-stone-700 dark:border-white text-stone-700 dark:text-white hover:text-stone-500 dark:hover:text-neutral-800 hover:bg-neutral-100">IonQ Vision Docs</a><a href="https://cloud.ionq.com" target="_blank" rel="noopener noreferrer" class="inline-block px-4 py-2 mx-1 mt-0 leading-none border rounded text-md border-stone-700 dark:border-white text-stone-700 dark:text-white hover:text-stone-500 dark:hover:text-neutral-800 hover:bg-neutral-100">IonQ Cloud</a></div></div></nav></div><div class="fixed xl:article-grid grid-gap xl:w-screen xl:pointer-events-none overflow-auto max-xl:min-w-[300px] hidden z-10" style="top:60px"><div class="pointer-events-auto xl:col-margin-left flex-col overflow-hidden hidden xl:flex"><nav aria-label="Table of Contents" class="flex-grow overflow-y-auto transition-opacity mt-6 pb-3 ml-3 xl:ml-0 mr-3 max-w-[350px]"><div class="w-full px-1 dark:text-white"><a title="Zero or one?" class="block break-words focus:outline outline-blue-200 outline-2 rounded p-2 my-1 rounded-lg hover:bg-slate-300/30 font-bold" href="/qml-challenge-scorer/">Zero or one?</a><a title="Your mission" class="block break-words focus:outline outline-blue-200 outline-2 rounded p-2 my-1 rounded-lg hover:bg-slate-300/30" href="/qml-challenge-scorer/challenge-description">Your mission</a><a title="Tips, tricks, and potential gotchas" class="block break-words focus:outline outline-blue-200 outline-2 rounded p-2 my-1 rounded-lg hover:bg-slate-300/30" href="/qml-challenge-scorer/details">Tips, tricks, and potential gotchas</a><a title="Environment set up" class="block break-words focus:outline outline-blue-200 outline-2 rounded p-2 my-1 rounded-lg hover:bg-slate-300/30" href="/qml-challenge-scorer/env-setup">Environment set up</a><a title="Leader board" class="block break-words focus:outline outline-blue-200 outline-2 rounded p-2 my-1 rounded-lg hover:bg-slate-300/30" href="/qml-challenge-scorer/leaderboard">Leader board</a></div></nav><div class="flex-none py-4 transition-all duration-700 translate-y-6 opacity-0"><a class="flex mx-auto text-gray-700 w-fit hover:text-blue-700 dark:text-gray-200 dark:hover:text-blue-400" href="https://mystmd.org/made-with-myst" target="_blank" rel="noreferrer"><svg style="width:24px;height:24px" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 100 100" stroke="none"><g id="icon"><path fill="currentColor" d="M23.8,54.8v-3.6l4.7-0.8V17.5l-4.7-0.8V13H36l13.4,31.7h0.2l13-31.7h12.6v3.6l-4.7,0.8v32.9l4.7,0.8v3.6h-15
          v-3.6l4.9-0.8V20.8H65L51.4,53.3h-3.8l-14-32.5h-0.1l0.2,17.4v12.1l5,0.8v3.6H23.8z"></path><path fill="#F37726" d="M47,86.9c0-5.9-3.4-8.8-10.1-8.8h-8.4c-5.2,0-9.4-1.3-12.5-3.8c-3.1-2.5-5.4-6.2-6.8-11l4.8-1.6
          c1.8,5.6,6.4,8.6,13.8,8.8h9.2c6.4,0,10.8,2.5,13.1,7.5c2.3-5,6.7-7.5,13.1-7.5h8.4c7.8,0,12.7-2.9,14.6-8.7l4.8,1.6
          c-1.4,4.9-3.6,8.6-6.8,11.1c-3.1,2.5-7.3,3.7-12.4,3.8H63c-6.7,0-10,2.9-10,8.8"></path></g></svg><span class="self-center ml-2 text-sm">Made with MyST</span></a></div></div></div><article class="article content article-grid grid-gap" style="margin-top:60px"><main class="article-grid subgrid-gap col-screen"><div class="hidden"></div><div id="skip-to-frontmatter" aria-label="article frontmatter" class="pt-5 mb-8"><div class="flex items-center h-6 mt-3 mb-5 text-sm font-light"><div class="flex-none pr-2 smallcaps border-r mr-2">Quantum Vision Challenge</div><div class="flex-none mr-2"><span class="font-semibold smallcaps">IonQ-SKKU Hackathon</span></div><div class="flex-grow"></div><a href="https://github.com/willieab/ionq-skku-vision-challenge" title="GitHub Repository: willieab/ionq-skku-vision-challenge" target="_blank" rel="noopener noreferrer" class="text-inherit hover:text-inherit"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" width="1.25rem" height="1.25rem" class="inline-block mr-1 opacity-60 hover:opacity-100"><path d="M12 2.5c-5.4 0-9.8 4.4-9.8 9.7 0 4.3 2.8 8 6.7 9.2.5.1.7-.2.7-.5v-1.8c-2.4.5-3.1-.6-3.3-1.1-.1-.3-.6-1.1-1-1.4-.3-.2-.8-.6 0-.6s1.3.7 1.5 1c.9 1.5 2.3 1.1 2.8.8.1-.6.3-1.1.6-1.3-2.2-.2-4.4-1.1-4.4-4.8 0-1.1.4-1.9 1-2.6-.1-.2-.4-1.2.1-2.6 0 0 .8-.3 2.7 1 .8-.2 1.6-.3 2.4-.3.8 0 1.7.1 2.4.3 1.9-1.3 2.7-1 2.7-1 .5 1.3.2 2.3.1 2.6.6.7 1 1.5 1 2.6 0 3.7-2.3 4.6-4.4 4.8.4.3.7.9.7 1.8V21c0 .3.2.6.7.5 3.9-1.3 6.6-4.9 6.6-9.2 0-5.4-4.4-9.8-9.8-9.8z"></path></svg></a><div class="inline-block mr-1"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" width="1.25rem" height="1.25rem" class="inline-block"><path d="M20.2 1.7c0 .8-.5 1.4-1.3 1.5-.8 0-1.4-.5-1.5-1.3 0-.8.5-1.4 1.3-1.5.8-.1 1.5.5 1.5 1.3zM12 17.9c-3.7 0-7-1.3-8.7-3.3 1.8 4.8 7.1 7.3 11.9 5.5 2.5-.9 4.5-2.9 5.5-5.5-1.7 2-4.9 3.3-8.7 3.3zM12 5.1c3.7 0 7 1.3 8.7 3.3-1.8-4.8-7.1-7.3-11.9-5.5-2.5.9-4.5 2.9-5.5 5.5 1.7-2 5-3.3 8.7-3.3zM6.9 21.8c.1 1-.7 1.8-1.7 1.9-1 .1-1.8-.7-1.9-1.7 0-1 .7-1.8 1.7-1.9 1-.1 1.8.7 1.9 1.7zM3.7 4.6c-.6 0-1-.4-1-1s.4-1 1-1 1 .4 1 1c0 .5-.4 1-1 1z"></path></svg></div><div class="relative flex inline-block mx-1 grow-0" data-headlessui-state=""><button class="relative ml-2 -mr-1" id="headlessui-menu-button-:Rd4fop:" type="button" aria-haspopup="menu" aria-expanded="false" data-headlessui-state=""><span class="sr-only">Downloads</span><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" width="1.25rem" height="1.25rem"><path stroke-linecap="round" stroke-linejoin="round" d="M3 16.5v2.25A2.25 2.25 0 005.25 21h13.5A2.25 2.25 0 0021 18.75V16.5M16.5 12L12 16.5m0 0L7.5 12m4.5 4.5V3"></path></svg></button></div></div><h1 class="mb-0">Your mission</h1><header class="mt-4 not-prose"><div><span class="font-semibold text-sm inline-block"><button class="focus:shadow-[0_0_0_2px] focus:shadow-black outline-none hover:underline" aria-label="Author Details" type="button" aria-haspopup="dialog" aria-expanded="false" aria-controls="radix-:R3kfop:" data-state="closed">Willie Aboumrad</button></span></div></header></div><div class="block my-10 lg:sticky lg:top-0 lg:z-10 lg:h-0 lg:pt-2 lg:my-0 lg:ml-10 lg:col-margin-right"><nav></nav></div><div id="skip-to-article"></div><div id="vl9fXwmDUA" class="relative group/block article-grid subgrid-gap col-screen"><p>In this challenge you’ll implement a hybrid quantum-classical image classification pipeline using <code>ionqvision</code>, IonQ’s next-generation platform for QML computations, and <a target="_blank" href="https://pytorch.org" rel="noreferrer"><code>PyTorch</code></a>. In particular, you’ll train a quantum-classical neural network to distinguish between two types of different handwritten digits in the <a href="https://en.wikipedia.org/wiki/MNIST_database" class="italic" target="_blank" rel="noreferrer" data-state="closed">MNIST database</a>.</p><aside class="my-5 shadow-md dark:shadow-2xl dark:shadow-neutral-900 bg-gray-50/10 dark:bg-stone-800 overflow-hidden rounded border-l-4 border-green-600"><div class="m-0 font-medium py-1 flex min-w-0 text-lg text-green-600 bg-green-50 dark:bg-slate-900"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" width="2rem" height="2rem" class="inline-block pl-2 mr-2 self-center flex-none text-green-600"><path stroke-linecap="round" stroke-linejoin="round" d="M12 18v-5.25m0 0a6.01 6.01 0 001.5-.189m-1.5.189a6.01 6.01 0 01-1.5-.189m3.75 7.478a12.06 12.06 0 01-4.5 0m3.75 2.383a14.406 14.406 0 01-3 0M14.25 18v-.192c0-.983.658-1.823 1.508-2.316a7.5 7.5 0 10-7.517 0c.85.493 1.509 1.333 1.509 2.316V18"></path></svg><div class="text-neutral-900 dark:text-white grow self-center overflow-hidden break-words">Tip</div></div><div class="px-4 py-1"><p>If you haven’t used <code>PyTorch</code> yet, don’t sweat it!</p><p>This challenge is about the <strong>quantum</strong> in <a href="https://en.wikipedia.org/wiki/Quantum_machine_learning" class="italic" target="_blank" rel="noreferrer" data-state="closed">“Quantum Machine Learning”</a>. The accompanying <span data-state="closed"><a href="#sec-training" class="hover-link">starter code</a></span> will set up everything you need, so you can focus on building your best quantum circuits!</p></div></aside><p>The figure below illustrates our hybrid pipeline. The operations inside the gray box make up the <em>quantum layer</em>. That’s where the magic happens.</p><figure id="fig-hybrid-nn" class="fig-figure"><img id="MCpiIo2EEc" style="margin:0 auto" src="/qml-challenge-scorer/build/hybrid_nn-02bee137d9c820208ac883f635840f61.png" alt="A diagrammatic representation of our hybrid quantum-classical image classification pipeline" data-canonical-url="media/hybrid_nn.png"/><figcaption class="group"><p><a class="select-none no-underline text-inherit hover:text-inherit mr-1 font-semibold text-inherit hover:text-inherit hover:font-semibold hover:underline" href="#fig-hybrid-nn" title="Link to this figure" aria-label="Link to this figure">Figure <!-- -->1<!-- -->:</a>A diagrammatic representation of our hybrid quantum-classical image classification pipeline</p></figcaption></figure><p>Your mission, should you choose to accept it, is to design the quantum layer that yields the <strong>highest possible classification accuracy</strong> (scored automatically on an unseen validation set). Simple, right?</p><h2 id="classical-quantum-and-then-classical-again" class="relative group"><span class="heading-text">Classical, quantum, and then classical again</span><a class="select-none no-underline text-inherit hover:text-inherit px-2 font-normal transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#classical-quantum-and-then-classical-again" title="Link to this Section" aria-label="Link to this Section">¶</a></h2><p>If you’ve made it thus far, congratulations! 🎉</p><p>You’re about to embark on an exciting journey of quantum discovery. To begin, we’ll take a closer look at the architecture of our hybrid neural network. It’s important to keep in mind that going through our full <em>hybrid</em> pipeline requires <strong>both</strong> classical and quantum hardware, and software that facilitates communication between the two. The <code>ionqvision</code> abstracts much of this complexity away and it let’s you focus on the quantum design.</p><p>In what follows, we’ll explain the classical and quantum parts of our pipeline in some detail. Then we’ll turn to the <span data-state="closed"><a href="#sec-training" class="hover-link">starter code</a></span>, which will get you up and running in no time!</p><h3 id="classical-pre-processing" class="relative group"><span class="heading-text">Classical pre-processing</span><a class="select-none no-underline text-inherit hover:text-inherit px-2 font-normal transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#classical-pre-processing" title="Link to this Section" aria-label="Link to this Section">¶</a></h3><p>The first three layers in our pipeline are purely classical. They implement fairly basic operations commonly encountered in classical image classification pipelines: first flatten each (single-channel) input image, perform dimensionality reduction using <a href="https://en.wikipedia.org/wiki/Principal_component_analysis" class="italic" target="_blank" rel="noreferrer" data-state="closed">PCA</a>, and then embed the compressed images into an even lower-dimensional feature space using a trainable fully-connected layer. We use drop out during training and the sigmoid <a href="https://en.wikipedia.org/wiki/Activation_function" class="italic" target="_blank" rel="noreferrer" data-state="closed">activation function</a>.</p><img id="WzCp2dCRbV" style="width:75%;margin:0 auto" src="/qml-challenge-scorer/build/classical_pre-96198977b57d82ee819e306155372f1c.png" data-canonical-url="media/classical_pre.png"/><aside class="my-5 shadow-md dark:shadow-2xl dark:shadow-neutral-900 bg-gray-50/10 dark:bg-stone-800 overflow-hidden rounded border-l-4 border-amber-600"><div class="m-0 font-medium py-1 flex min-w-0 text-lg text-amber-600 bg-amber-50 dark:bg-slate-900"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" width="2rem" height="2rem" class="inline-block pl-2 mr-2 self-center flex-none text-amber-600"><path fill-rule="evenodd" d="M9.401 3.003c1.155-2 4.043-2 5.197 0l7.355 12.748c1.154 2-.29 4.5-2.599 4.5H4.645c-2.309 0-3.752-2.5-2.598-4.5L9.4 3.003zM12 8.25a.75.75 0 01.75.75v3.75a.75.75 0 01-1.5 0V9a.75.75 0 01.75-.75zm0 8.25a.75.75 0 100-1.5.75.75 0 000 1.5z" clip-rule="evenodd"></path></svg><div class="text-neutral-900 dark:text-white grow self-center overflow-hidden break-words">Note</div></div><div class="px-4 py-1"><p>This part of the architecture is essentially <strong>fixed</strong>: the only thing you get to decide here is the dimension <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">n</span></span></span></span></span> of the resulting latent vectors. For instance, in <span data-state="closed"><a href="#fig-hybrid-nn" class="hover-link">Figure <!-- -->1</a></span>, <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>=</mo><mn>12</mn></mrow><annotation encoding="application/x-tex">n = 12</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">n</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">12</span></span></span></span></span>.</p></div></aside><h3 id="sec-quantum-layer" class="relative group"><span class="heading-text">Quantum Layer</span><a class="select-none no-underline text-inherit hover:text-inherit px-2 font-normal transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#sec-quantum-layer" title="Link to this Section" aria-label="Link to this Section">¶</a></h3><p>Then comes the most interesting part: the “quantum layer.” Our whole challenge revolves around this layer: the best design here will achieve the highest classification accuracy. And there’s a <strong>big prize</strong> for that. 🥇</p><p>The quantum layer consists of three components:</p><ol start="1"><li>an encoding circuit used to load each <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">n</span></span></span></span></span>-dimensional latent vector into an <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">n</span></span></span></span></span>-dimensional qubit state,</li><li>a variational circuit on <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">n</span></span></span></span></span> qubits with trainable parameters used to learn how to separate encoded latent vectors in the high-dimensional multi-qubit state space, and</li><li>a list of <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi></mrow><annotation encoding="application/x-tex">m</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">m</span></span></span></span></span> measureable quantities used to extract “quantum features” from the transformed <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">n</span></span></span></span></span>-qubit states for classification. (For instance, <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mo>=</mo><mn>4</mn></mrow><annotation encoding="application/x-tex">m = 4</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">m</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">4</span></span></span></span></span> in <span data-state="closed"><a href="#fig-hybrid-nn" class="hover-link">Figure <!-- -->1</a></span>.)</li></ol><img id="td2rQd0KSj" style="width:75%;margin:0 auto" src="/qml-challenge-scorer/build/quantum_layer-798e7871a9d7e580cdba7345274c5ce4.png" data-canonical-url="media/quantum_layer.png"/><p>To get the ball rolling, look to the <code>ionqvision.ansatze.ansatz_library</code> module for inpiration. We’ve implemented some of the encoders and ansatze commonly encountered in the literature.</p><p>For instance, you could use the <code>AngleEncoder</code>, which has the following structure, for your first model.</p></div><div id="uj3clRVymo" class="relative group/block article-grid subgrid-gap col-screen"><div class="flex sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:hidden"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="hidden sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="relative group not-prose overflow-auto shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 my-5 text-sm border border-l-4 border-l-blue-400 border-gray-200 dark:border-l-blue-400 dark:border-gray-800"><pre style="display:block;overflow-x:auto;padding:0.8rem;background:transparent;color:black"><code class="language-python" style="white-space:pre">from ionqvision.ansatze.ansatz_library import AngleEncoder

encoder = AngleEncoder(num_qubits=4)
encoder.draw(&quot;mpl&quot;)</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 01-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 011.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 00-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 01-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 00-3.375-3.375h-1.5a1.125 1.125 0 01-1.125-1.125v-1.5a3.375 3.375 0 00-3.375-3.375H9.75"></path></svg></button></div><div data-mdast-node-id="PAwV1KcfRy4yisjlNhpBu" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><img src="/qml-challenge-scorer/build/6f193422c8d03a43328e34cf4782af51.png" alt="&lt;Figure size 454.719x367.889 with 1 Axes&gt;"/></div></div><div id="slaSkoLr94" class="relative group/block article-grid subgrid-gap col-screen"><p>In addition, you can use its implementation as a <strong>template</strong> for your novel encoder designs!</p><div class="relative group not-prose overflow-auto shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 my-5 text-sm bg-stone-200/10"><div class="flex flex-row pl-2 bg-white border-b dark:bg-slate-600 dark:border-slate-300"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" width="16px" height="16px" class="self-center flex-none inline-block text-gray-500 dark:text-gray-100"><path stroke-linecap="round" stroke-linejoin="round" d="M19.5 14.25v-2.625a3.375 3.375 0 00-3.375-3.375h-1.5A1.125 1.125 0 0113.5 7.125v-1.5a3.375 3.375 0 00-3.375-3.375H8.25m2.25 0H5.625c-.621 0-1.125.504-1.125 1.125v17.25c0 .621.504 1.125 1.125 1.125h12.75c.621 0 1.125-.504 1.125-1.125V11.25a9 9 0 00-9-9z"></path></svg><div class="self-center p-2 text-sm leading-3 prose text-slate-600 dark:text-white">ansatz_library.py</div></div><pre style="display:block;overflow-x:auto;padding:0.8rem;background:transparent;color:black"><code style="display:inline-block;float:left;min-width:1.25em;padding-right:1em;text-align:right;user-select:none;border-left:4px solid transparent"><span class="react-syntax-highlighter-line-number">19
</span><span class="react-syntax-highlighter-line-number">20
</span><span class="react-syntax-highlighter-line-number">21
</span><span class="react-syntax-highlighter-line-number">22
</span><span class="react-syntax-highlighter-line-number">23
</span><span class="react-syntax-highlighter-line-number">24
</span><span class="react-syntax-highlighter-line-number">25
</span><span class="react-syntax-highlighter-line-number">26
</span><span class="react-syntax-highlighter-line-number">27
</span><span class="react-syntax-highlighter-line-number">28
</span><span class="react-syntax-highlighter-line-number">29
</span><span class="react-syntax-highlighter-line-number">30
</span><span class="react-syntax-highlighter-line-number">31
</span><span class="react-syntax-highlighter-line-number">32
</span><span class="react-syntax-highlighter-line-number">33
</span><span class="react-syntax-highlighter-line-number">34
</span><span class="react-syntax-highlighter-line-number">35
</span><span class="react-syntax-highlighter-line-number">36
</span><span class="react-syntax-highlighter-line-number">37
</span><span class="react-syntax-highlighter-line-number">38
</span><span class="react-syntax-highlighter-line-number">39
</span><span class="react-syntax-highlighter-line-number">40
</span><span class="react-syntax-highlighter-line-number">41
</span><span class="react-syntax-highlighter-line-number">42
</span><span class="react-syntax-highlighter-line-number">43
</span><span class="react-syntax-highlighter-line-number">44
</span><span class="react-syntax-highlighter-line-number">45
</span><span class="react-syntax-highlighter-line-number">46
</span><span class="react-syntax-highlighter-line-number">47
</span><span class="react-syntax-highlighter-line-number">48
</span><span class="react-syntax-highlighter-line-number">49
</span><span class="react-syntax-highlighter-line-number">50
</span><span class="react-syntax-highlighter-line-number">51
</span><span class="react-syntax-highlighter-line-number">52
</span><span class="react-syntax-highlighter-line-number">53
</span><span class="react-syntax-highlighter-line-number">54
</span><span class="react-syntax-highlighter-line-number">55
</span><span class="react-syntax-highlighter-line-number">56
</span><span class="react-syntax-highlighter-line-number">57
</span></code><code class="language-python" style="white-space:pre">class AngleEncoder(VariationalAnsatz):
    &quot;&quot;&quot;
    Implement a quantum circuit for higher-order sparse angle encoding.

    INPUT:

        - ``num_qubits`` -- number of qubits
        - ``entanglement_depth`` -- (optional) number layers of entangling CNOT
          gates: for each ``k`` in ``range(entanglement_depth)``, use gates
          ``CNOT(j, j + k + 1)``.
        - ``param_prefix`` -- (optional) string prefix for named circuit
          parameters

    EXAMPLES::

        &gt;&gt;&gt; from ionqvision.ansatze.ansatz_library import AngleEncoder
        &gt;&gt;&gt; ansatz = AngleEncoder(4, entanglement_depth=3, param_prefix=&quot;y&quot;)
        &gt;&gt;&gt; ansatz.draw()
             ┌────────────┐                              
        q_0: ┤ Ry(π*y[0]) ├──■──────────────■─────────■──
             ├────────────┤┌─┴─┐            │         │  
        q_1: ┤ Ry(π*y[1]) ├┤ X ├──■─────────┼────■────┼──
             ├────────────┤└───┘┌─┴─┐     ┌─┴─┐  │    │  
        q_2: ┤ Ry(π*y[2]) ├─────┤ X ├──■──┤ X ├──┼────┼──
             ├────────────┤     └───┘┌─┴─┐└───┘┌─┴─┐┌─┴─┐
        q_3: ┤ Ry(π*y[3]) ├──────────┤ X ├─────┤ X ├┤ X ├
             └────────────┘          └───┘     └───┘└───┘
    &quot;&quot;&quot;
    def __init__(self, num_qubits, entanglement_depth=1, param_prefix=&quot;x&quot;):
        super().__init__(num_qubits)

        x = ParameterVector(param_prefix, num_qubits)
        [self.ry(np.pi * xi, qbt) for qbt, xi in enumerate(x)]

        for k in range(entanglement_depth):
            top_qubit = 0
            while top_qubit &lt; num_qubits - (k + 1):
                self.cx(top_qubit, top_qubit + k + 1)
                top_qubit += 1</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 top-[32px]" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 01-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 011.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 00-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 01-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 00-3.375-3.375h-1.5a1.125 1.125 0 01-1.125-1.125v-1.5a3.375 3.375 0 00-3.375-3.375H9.75"></path></svg></button></div><p>Similarly, you can leverage the built-in <code>BrickworkLayoutAnsatz</code>, or the <code>QCNNAnsatz</code>, amongst others, for the trainable layer.</p></div><div id="GW0kjF5Tom" class="relative group/block article-grid subgrid-gap col-screen"><div class="flex sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:hidden"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="hidden sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="relative group not-prose overflow-auto shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 my-5 text-sm border border-l-4 border-l-blue-400 border-gray-200 dark:border-l-blue-400 dark:border-gray-800"><pre style="display:block;overflow-x:auto;padding:0.8rem;background:transparent;color:black"><code class="language-python" style="white-space:pre">from ionqvision.ansatze.ansatz_library import QCNNAnsatz

ansatz = QCNNAnsatz(num_qubits=4)
ansatz.draw(&quot;mpl&quot;)</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 01-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 011.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 00-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 01-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 00-3.375-3.375h-1.5a1.125 1.125 0 01-1.125-1.125v-1.5a3.375 3.375 0 00-3.375-3.375H9.75"></path></svg></button></div><div data-mdast-node-id="9P3QdrUGkEPCBWmwVcAdJ" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><img src="/qml-challenge-scorer/build/2b2c08850ab12a480f4662f2618dec5a.png" alt="&lt;Figure size 1207.22x367.889 with 1 Axes&gt;"/></div></div><div id="xNY1bkmmeb" class="relative group/block article-grid subgrid-gap col-screen"><p>This ansatz is particularly interesting as it consists of a sequence of “convolution” filters interspersed with pooling operations that reduce the number of “active” qubits upon each layer. Note that our implemenation modifies the <a target="_blank" href="https://www.nature.com/articles/s41567-019-0648-8" rel="noreferrer">original design</a> by replacing the mid-circuit measurements with controlled rotations.</p><p>If you’d like to try out this ansatz, be sure to <a target="_blank" href="https://pybit.es/articles/python-subclasses/" rel="noreferrer">subclass</a> and implement your own <code>QCNNAnsatz.ConvolutionBrickwork</code> and <code>QCNN.PoolingLayer</code> modules!</p><div class="relative group not-prose overflow-auto shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 my-5 text-sm bg-stone-200/10"><div class="flex flex-row pl-2 bg-white border-b dark:bg-slate-600 dark:border-slate-300"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" width="16px" height="16px" class="self-center flex-none inline-block text-gray-500 dark:text-gray-100"><path stroke-linecap="round" stroke-linejoin="round" d="M19.5 14.25v-2.625a3.375 3.375 0 00-3.375-3.375h-1.5A1.125 1.125 0 0113.5 7.125v-1.5a3.375 3.375 0 00-3.375-3.375H8.25m2.25 0H5.625c-.621 0-1.125.504-1.125 1.125v17.25c0 .621.504 1.125 1.125 1.125h12.75c.621 0 1.125-.504 1.125-1.125V11.25a9 9 0 00-9-9z"></path></svg><div class="self-center p-2 text-sm leading-3 prose text-slate-600 dark:text-white">ansatz_library.py</div></div><pre style="display:block;overflow-x:auto;padding:0.8rem;background:transparent;color:black"><code style="display:inline-block;float:left;min-width:1.25em;padding-right:1em;text-align:right;user-select:none;border-left:4px solid transparent"><span class="react-syntax-highlighter-line-number">271
</span><span class="react-syntax-highlighter-line-number">272
</span><span class="react-syntax-highlighter-line-number">273
</span><span class="react-syntax-highlighter-line-number">274
</span><span class="react-syntax-highlighter-line-number">275
</span><span class="react-syntax-highlighter-line-number">276
</span><span class="react-syntax-highlighter-line-number">277
</span><span class="react-syntax-highlighter-line-number">278
</span><span class="react-syntax-highlighter-line-number">279
</span><span class="react-syntax-highlighter-line-number">280
</span><span class="react-syntax-highlighter-line-number">281
</span><span class="react-syntax-highlighter-line-number">282
</span><span class="react-syntax-highlighter-line-number">283
</span><span class="react-syntax-highlighter-line-number">284
</span><span class="react-syntax-highlighter-line-number">285
</span><span class="react-syntax-highlighter-line-number">286
</span><span class="react-syntax-highlighter-line-number">287
</span><span class="react-syntax-highlighter-line-number">288
</span><span class="react-syntax-highlighter-line-number">289
</span><span class="react-syntax-highlighter-line-number">290
</span><span class="react-syntax-highlighter-line-number">291
</span><span class="react-syntax-highlighter-line-number">292
</span><span class="react-syntax-highlighter-line-number">293
</span><span class="react-syntax-highlighter-line-number">294
</span><span class="react-syntax-highlighter-line-number">295
</span><span class="react-syntax-highlighter-line-number">296
</span><span class="react-syntax-highlighter-line-number">297
</span><span class="react-syntax-highlighter-line-number">298
</span><span class="react-syntax-highlighter-line-number">299
</span><span class="react-syntax-highlighter-line-number">300
</span><span class="react-syntax-highlighter-line-number">301
</span><span class="react-syntax-highlighter-line-number">302
</span><span class="react-syntax-highlighter-line-number">303
</span><span class="react-syntax-highlighter-line-number">304
</span><span class="react-syntax-highlighter-line-number">305
</span><span class="react-syntax-highlighter-line-number">306
</span><span class="react-syntax-highlighter-line-number">307
</span><span class="react-syntax-highlighter-line-number">308
</span><span class="react-syntax-highlighter-line-number">309
</span><span class="react-syntax-highlighter-line-number">310
</span><span class="react-syntax-highlighter-line-number">311
</span><span class="react-syntax-highlighter-line-number">312
</span><span class="react-syntax-highlighter-line-number">313
</span><span class="react-syntax-highlighter-line-number">314
</span><span class="react-syntax-highlighter-line-number">315
</span><span class="react-syntax-highlighter-line-number">316
</span><span class="react-syntax-highlighter-line-number">317
</span><span class="react-syntax-highlighter-line-number">318
</span><span class="react-syntax-highlighter-line-number">319
</span><span class="react-syntax-highlighter-line-number">320
</span><span class="react-syntax-highlighter-line-number">321
</span><span class="react-syntax-highlighter-line-number">322
</span><span class="react-syntax-highlighter-line-number">323
</span><span class="react-syntax-highlighter-line-number">324
</span><span class="react-syntax-highlighter-line-number">325
</span><span class="react-syntax-highlighter-line-number">326
</span><span class="react-syntax-highlighter-line-number">327
</span><span class="react-syntax-highlighter-line-number">328
</span><span class="react-syntax-highlighter-line-number">329
</span><span class="react-syntax-highlighter-line-number">330
</span></code><code class="language-python" style="white-space:pre">class QCNNAnsatz(VariationalAnsatz):
    r&quot;&quot;&quot;
    Implement the Quantum Convolutional Network Ansatz (QCNN) as described in
    :cite:t:`2019:qcnn`.

    The quasi-local unitary $U_i$&#x27;s are entangling two-qubit gates with $6$
    variational parameters.
    They are laid out in a brickwork pattern with ``filter_depth`` layers.

    The pooling operations are implemented by two-qubit controlled rotations,
    with $2$ variational parameters.

    The circuit starts with ``num_qubits`` active qubits and then half the
    remaining qubits are discarded after each pooling operation until only a
    single active qubit remains. This final qubit is measured and the result is
    used for binary classification.
    &quot;&quot;&quot;
    class ConvolutionBrickwork(BrickworkLayoutAnsatz):
        &quot;&quot;&quot;
        Implement the convolution filters for the :class:`.QCNNAnsatz`.
        &quot;&quot;&quot;
        def __init__(self, num_qubits, num_layers, prefix=None, qubits=None, initial_state=None):
            super().__init__(num_qubits, num_layers, blk_sz=3, prefix=prefix, qubits=qubits, initial_state=initial_state)
        
        def two_qubit_block(self, theta, q1, q2):
            conv_op = QuantumCircuit(2, name=&quot;CONV&quot;)
            conv_op.ry(theta[0], 0)
            conv_op.ry(theta[1], 1)
            conv_op.rxx(theta[2], 0, 1)
            self.append(conv_op.to_instruction(), [q1, q2])

    class PoolingLayer(BrickworkLayoutAnsatz):
        &quot;&quot;&quot;
        Implement the pooling layer for the :class:`.QCNNAnsatz`.
        &quot;&quot;&quot;
        def __init__(self, num_qubits, prefix=None, qubits=None):
            super().__init__(num_qubits, 1, blk_sz=1, prefix=prefix, qubits=qubits)
    
        def two_qubit_block(self, theta, q1, q2):
            pool_op = QuantumCircuit(2, name=&quot;POOL&quot;)
            pool_op.crz(theta[0], 1, 0)
            self.append(pool_op.to_instruction(), [q1, q2])

    def __init__(self, num_qubits, filter_depth=2, initial_state=None):
        num_layers = int(log(num_qubits, 2))
        if abs(log(num_qubits, 2) - num_layers) &gt; 1e-6:
            raise ValueError(&quot;num_qubits must be a power of 2&quot;)

        super().__init__(num_qubits)
        if initial_state is not None:
            self.compose(initial_state, inplace=True)

        for k in range(num_layers):
            qubits = list(range(0, num_qubits, 2**k))
        
            conv = QCNNAnsatz.ConvolutionBrickwork(num_qubits, filter_depth, prefix=&quot;C&quot; + str(k), qubits=qubits)
            self.compose(conv, inplace=True)
            
            pool = QCNNAnsatz.PoolingLayer(num_qubits, prefix=&quot;P&quot; + str(k), qubits=qubits)
            self.compose(pool, inplace=True)</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 top-[32px]" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 01-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 011.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 00-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 01-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 00-3.375-3.375h-1.5a1.125 1.125 0 01-1.125-1.125v-1.5a3.375 3.375 0 00-3.375-3.375H9.75"></path></svg></button></div><p>While <code>ionqvision</code> does not provide built-in qubit observables, you can set these up using <code>qiskit</code> as follows.</p></div><div id="rwhyvR8bpq" class="relative group/block article-grid subgrid-gap col-screen"><div class="flex sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:hidden"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="hidden sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="relative group not-prose overflow-auto shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 my-5 text-sm border border-l-4 border-l-blue-400 border-gray-200 dark:border-l-blue-400 dark:border-gray-800"><pre style="display:block;overflow-x:auto;padding:0.8rem;background:transparent;color:black"><code class="language-python" style="white-space:pre">from qiskit.quantum_info import SparsePauliOp

# Measure the expectation value of X_0, Y_0, Z_0
quantum_features = [
    SparsePauliOp([&quot;IIIX&quot;]), 
    SparsePauliOp([&quot;IIIY&quot;]), 
    SparsePauliOp([&quot;IIIZ&quot;])
]</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 01-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 011.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 00-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 01-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 00-3.375-3.375h-1.5a1.125 1.125 0 01-1.125-1.125v-1.5a3.375 3.375 0 00-3.375-3.375H9.75"></path></svg></button></div><div data-mdast-node-id="sGlnzc61QCaSthAFrHu7u" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left"></div></div><div id="ATVjAxLv87" class="relative group/block article-grid subgrid-gap col-screen"><p>It’s important to keep in mind that the encoder, the ansatz, and the quantum feature vector are highly interrelated: the way you embed latent vectors into (multi-)qubit state space should dictate how you choose to transform the encoded state vectors, which should in turn inform what features you decide to measure.</p><p>The best model will likely exploit synergies resulting from intentional co-design of the three components.</p><aside class="my-5 shadow-md dark:shadow-2xl dark:shadow-neutral-900 bg-gray-50/10 dark:bg-stone-800 overflow-hidden rounded border-l-4 border-blue-500"><div class="m-0 font-medium py-1 flex min-w-0 text-lg text-blue-600 bg-blue-50 dark:bg-slate-900"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" width="2rem" height="2rem" class="inline-block pl-2 mr-2 self-center flex-none text-blue-600"><path fill-rule="evenodd" d="M14.615 1.595a.75.75 0 01.359.852L12.982 9.75h7.268a.75.75 0 01.548 1.262l-10.5 11.25a.75.75 0 01-1.272-.71l1.992-7.302H3.75a.75.75 0 01-.548-1.262l10.5-11.25a.75.75 0 01.913-.143z" clip-rule="evenodd"></path></svg><div class="text-neutral-900 dark:text-white grow self-center overflow-hidden break-words">Important</div></div><div class="px-4 py-1"><p>This is where you should let your imagination run free! Get creative, and show us what you’ve got. Feel free to leverage the power of the internet and use every resource at your disposal.</p></div></aside><h3 id="classical-post-processing" class="relative group"><span class="heading-text">Classical post-processing</span><a class="select-none no-underline text-inherit hover:text-inherit px-2 font-normal transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#classical-post-processing" title="Link to this Section" aria-label="Link to this Section">¶</a></h3><p>After the quantum layer, feature vectors in the classification pipeline return to the classical device for some post-processing. In particular, we train a fully-connected layer with a scalar output to minimize the binary <a href="https://en.wikipedia.org/wiki/Cross-entropy" class="italic" target="_blank" rel="noreferrer" data-state="closed">cross-entropy</a> between the final value and the input image’s true label. Again, we use drop out during training and the sigmoid <a href="https://en.wikipedia.org/wiki/Activation_function" class="italic" target="_blank" rel="noreferrer" data-state="closed">activation function</a>.</p><img id="ntA1vosyL2" style="width:45%;margin:0 auto" src="/qml-challenge-scorer/build/classical_post-af3006acd6306ca61c870ff9530707a4.png" data-canonical-url="media/classical_post.png"/><p>This final stage is intentionally light, to ensure the quantum layer is the star of the show.</p><aside class="my-5 shadow-md dark:shadow-2xl dark:shadow-neutral-900 bg-gray-50/10 dark:bg-stone-800 overflow-hidden rounded border-l-4 border-amber-600"><div class="m-0 font-medium py-1 flex min-w-0 text-lg text-amber-600 bg-amber-50 dark:bg-slate-900"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" width="2rem" height="2rem" class="inline-block pl-2 mr-2 self-center flex-none text-amber-600"><path fill-rule="evenodd" d="M9.401 3.003c1.155-2 4.043-2 5.197 0l7.355 12.748c1.154 2-.29 4.5-2.599 4.5H4.645c-2.309 0-3.752-2.5-2.598-4.5L9.4 3.003zM12 8.25a.75.75 0 01.75.75v3.75a.75.75 0 01-1.5 0V9a.75.75 0 01.75-.75zm0 8.25a.75.75 0 100-1.5.75.75 0 000 1.5z" clip-rule="evenodd"></path></svg><div class="text-neutral-900 dark:text-white grow self-center overflow-hidden break-words">Note</div></div><div class="px-4 py-1"><p>This part of the architecture is totally <strong>fixed</strong>: the input dimension (<span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi></mrow><annotation encoding="application/x-tex">m</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">m</span></span></span></span></span>) is determined by the quantum feature vector, and the output is always a scalar.</p></div></aside><h2 id="sec-training" class="relative group"><span class="heading-text">Training your model</span><a class="select-none no-underline text-inherit hover:text-inherit px-2 font-normal transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#sec-training" title="Link to this Section" aria-label="Link to this Section">¶</a></h2><p>Once you’ve settled on your quantum layer, you can sit back, relax, and let <code>ionqvision</code> do the heavy lifting. First, set up your classifier and verify it’s working as expected.</p></div><div id="VkQMbYZV1H" class="relative group/block article-grid subgrid-gap col-screen"><div class="flex sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:hidden"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="hidden sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="relative group not-prose overflow-auto shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 my-5 text-sm border border-l-4 border-l-blue-400 border-gray-200 dark:border-l-blue-400 dark:border-gray-800"><pre style="display:block;overflow-x:auto;padding:0.8rem;background:transparent;color:black"><code class="language-python" style="white-space:pre">from ionqvision.modules import BinaryMNISTClassifier

# Set up your classifier and inspect its architecture
classifier = BinaryMNISTClassifier(encoder, ansatz, quantum_features); classifier</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 01-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 011.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 00-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 01-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 00-3.375-3.375h-1.5a1.125 1.125 0 01-1.125-1.125v-1.5a3.375 3.375 0 00-3.375-3.375H9.75"></path></svg></button></div><div data-mdast-node-id="LK95_dc27ud9VqkiRPD67" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div class="font-mono text-sm whitespace-pre-wrap"><code><span>BinaryMNISTClassifier(
  (latent_vec_encoder): Sequential(
    (0): Linear(in_features=168, out_features=4, bias=True)
    (1): Dropout(p=0.5, inplace=False)
    (2): Sigmoid()
  )
  (quantum_layer): QuantumModule()
  (prediction_head): Sequential(
    (0): Linear(in_features=3, out_features=1, bias=True)
    (1): Dropout(p=0.5, inplace=False)
    (2): Sigmoid()
  )
)</span></code></div></div></div><div id="zBthROuGYh" class="relative group/block article-grid subgrid-gap col-screen"><p>In case you’re wondering, <code>BinaryMNISTClassifier</code> is a standard <code>torch.nn.Module</code>; at this point, all the parameters in your quantum layer have been registered with <code>torch</code> and the <code>autograd</code> will automatically compute the relevant gradients during the backward pass. Nothing else to worry about!</p><p>Be sure to checkout the <a target="_blank" href="https://willieab.github.io/qml-challenge-scorer/ionqvision-docs/" rel="noreferrer">IonQ Vision Docs</a> to learn more about the inner workings of the <code>BinaryMNISTClassifier</code> and all the related classes. The <kbd>IonQ Vision Docs</kbd> button at the top-right corner of the page links directly to the API refrence.</p></div><div id="EQnyuLMpPL" class="relative group/block article-grid subgrid-gap col-screen"><div class="flex sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:hidden"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="hidden sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="relative group not-prose overflow-auto shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 my-5 text-sm border border-l-4 border-l-blue-400 border-gray-200 dark:border-l-blue-400 dark:border-gray-800"><pre style="display:block;overflow-x:auto;padding:0.8rem;background:transparent;color:black"><code class="language-python" style="white-space:pre"># Check out your quantum layer
classifier.quantum_layer.layer_qc.draw(&quot;mpl&quot;)</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 01-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 011.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 00-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 01-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 00-3.375-3.375h-1.5a1.125 1.125 0 01-1.125-1.125v-1.5a3.375 3.375 0 00-3.375-3.375H9.75"></path></svg></button></div><div data-mdast-node-id="E3G-UWWk-pYAiI5orPoqM" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><img src="/qml-challenge-scorer/build/9050e03b17729dfdcbd08980b67ab512.png" alt="&lt;Figure size 1625.27x367.889 with 1 Axes&gt;"/></div></div><div id="KItthaMBV2" class="relative group/block article-grid subgrid-gap col-screen"><div class="flex sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:hidden"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="hidden sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="relative group not-prose overflow-auto shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 my-5 text-sm border border-l-4 border-l-blue-400 border-gray-200 dark:border-l-blue-400 dark:border-gray-800"><pre style="display:block;overflow-x:auto;padding:0.8rem;background:transparent;color:black"><code class="language-python" style="white-space:pre"># Verify the images loaded correctly
classifier.visualize_batch()</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 01-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 011.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 00-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 01-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 00-3.375-3.375h-1.5a1.125 1.125 0 01-1.125-1.125v-1.5a3.375 3.375 0 00-3.375-3.375H9.75"></path></svg></button></div><div data-mdast-node-id="1fbIeNVuuliE5x6HCU2qF" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div><pre class="text-sm font-thin font-system"><code><span>Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [0.28789353..1.9107434].
</span></code></pre></div><img src="/qml-challenge-scorer/build/4141f44566f3144c593db2cf7f36e0c7.png" alt="&lt;Figure size 640x480 with 1 Axes&gt;"/></div></div><div id="N0HYYLpjUi" class="relative group/block article-grid subgrid-gap col-screen"><p>Now train your model. Use the <code>config</code> dictionary to control lower-level aspects of the training, like the number of <code>epochs</code>, the learning rate, the <code>betas</code> used by <a target="_blank" href="https://pytorch.org/docs/stable/generated/torch.optim.Adam.html" rel="noreferrer">Adam</a>, etc.</p></div><div id="bMkohffFXx" class="relative group/block article-grid subgrid-gap col-screen"><div class="flex sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:hidden"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="hidden sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="relative group not-prose overflow-auto shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 my-5 text-sm border border-l-4 border-l-blue-400 border-gray-200 dark:border-l-blue-400 dark:border-gray-800"><pre style="display:block;overflow-x:auto;padding:0.8rem;background:transparent;color:black"><code class="language-python" style="white-space:pre"># Get a (pre-processed) training and test set
train_set, test_set = classifier.get_train_test_set(train_size=300, test_size=100)

# Configure model training hyper parameters
config = {
    &quot;epochs&quot;: 4,
    &quot;lr&quot;: 0.1,
    &quot;batch_size&quot;: 50,
    &quot;betas&quot;: (0.9, 0.99),
    &quot;weight_decay&quot;: 1e-3,
    &quot;clip_grad&quot;: True,
    &quot;log_interval&quot;: 6,
}

# Train and plot the results
classifier.train_module(train_set, test_set, config)
classifier.plot_training_progress()</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 01-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 011.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 00-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 01-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 00-3.375-3.375h-1.5a1.125 1.125 0 01-1.125-1.125v-1.5a3.375 3.375 0 00-3.375-3.375H9.75"></path></svg></button></div><div data-mdast-node-id="rDurOdPAll2BIL9iNCeR0" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div><pre class="text-sm font-thin font-system"><code><span>epoch:   1 | loss: 0.714
lr: 0.1000 | processed     6/    6 batches per epoch in 140.31s (0.76s forward / 21.37s backward)
Model achieved 54.667%  accuracy on TRAIN set.
Model achieved 53.000%  accuracy on TEST set.

epoch:   2 | loss: 0.698
lr: 0.1000 | processed     6/    6 batches per epoch in 140.33s (0.78s forward / 21.67s backward)
Model achieved 45.333%  accuracy on TRAIN set.
Model achieved 47.000%  accuracy on TEST set.

epoch:   3 | loss: 0.690
lr: 0.1000 | processed     6/    6 batches per epoch in 140.98s (0.80s forward / 22.25s backward)
Model achieved 65.000%  accuracy on TRAIN set.
Model achieved 57.000%  accuracy on TEST set.

epoch:   4 | loss: 0.695
lr: 0.1000 | processed     6/    6 batches per epoch in 139.55s (0.74s forward / 21.01s backward)
Model achieved 66.000%  accuracy on TRAIN set.
Model achieved 61.000%  accuracy on TEST set.

</span></code></pre></div><img src="/qml-challenge-scorer/build/f959aaa4c3821ca0035df9a894da1bdd.png" alt="&lt;Figure size 640x480 with 1 Axes&gt;"/></div></div><div id="uAssdkCI1E" class="relative group/block article-grid subgrid-gap col-screen"><p>The model achieved <span class="border-b border-dotted cursor-help" data-state="closed">&#x27;66.00%&#x27;</span> on the <strong>training</strong> set and <span class="border-b border-dotted cursor-help" data-state="closed">&#x27;61.00%&#x27;</span> on the <strong>test</strong> set.</p><h3 id="submitting-your-model-for-grading" class="relative group"><span class="heading-text">Submitting your model for grading</span><a class="select-none no-underline text-inherit hover:text-inherit px-2 font-normal transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="#submitting-your-model-for-grading" title="Link to this Section" aria-label="Link to this Section">¶</a></h3><p>Now comes the final thrill. Use your <code>classifier</code>’s <code>submit_model_for_grading</code> method to, well, submit your model for grading!</p><p>Your quantum layer and your model’s trained weights will be serialized and then reconstructed in a sanitized environment. We’ll run inference using the reconstructed model on an unseen validation set to compute your accuracy score.</p><p>Be sure to check our live leader board after a few minutes to see where your team stands!</p><aside class="my-5 shadow-md dark:shadow-2xl dark:shadow-neutral-900 bg-gray-50/10 dark:bg-stone-800 overflow-hidden rounded border-l-4 border-amber-600"><div class="m-0 font-medium py-1 flex min-w-0 text-lg text-amber-600 bg-amber-50 dark:bg-slate-900"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" width="2rem" height="2rem" class="inline-block pl-2 mr-2 self-center flex-none text-amber-600"><path fill-rule="evenodd" d="M9.401 3.003c1.155-2 4.043-2 5.197 0l7.355 12.748c1.154 2-.29 4.5-2.599 4.5H4.645c-2.309 0-3.752-2.5-2.598-4.5L9.4 3.003zM12 8.25a.75.75 0 01.75.75v3.75a.75.75 0 01-1.5 0V9a.75.75 0 01.75-.75zm0 8.25a.75.75 0 100-1.5.75.75 0 000 1.5z" clip-rule="evenodd"></path></svg><div class="text-neutral-900 dark:text-white grow self-center overflow-hidden break-words">Warning</div></div><div class="px-4 py-1"><p>Make sure you’ve created your team branch, e.g., using <code>git checkout -b my-team-name</code> before running the following command.</p><aside class="my-5 shadow-md dark:shadow-2xl dark:shadow-neutral-900 bg-gray-50/10 dark:bg-stone-800 overflow-hidden rounded border-l-4 border-green-600"><div class="m-0 font-medium py-1 flex min-w-0 text-lg text-green-600 bg-green-50 dark:bg-slate-900"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" width="2rem" height="2rem" class="inline-block pl-2 mr-2 self-center flex-none text-green-600"><path stroke-linecap="round" stroke-linejoin="round" d="M12.75 15l3-3m0 0l-3-3m3 3h-7.5M21 12a9 9 0 11-18 0 9 9 0 0118 0z"></path></svg><div class="text-neutral-900 dark:text-white grow self-center overflow-hidden break-words">See Also</div></div><div class="px-4 py-1"><p>Check out our <a href="/qml-challenge-scorer/env-setup">setup instructions</a> and our <a href="/qml-challenge-scorer/details">tips and tricks section</a> for more <span data-state="closed"><a class="hover-link" href="/qml-challenge-scorer/details#block-team-branch">details</a></span>.</p></div></aside><p>In addition, the following command assumes your current directory is the <strong>root</strong> of the cloned challenge repo (<code>ionq-skku-vision-challenge</code>). Check out the <a target="_blank" href="https://willieab.github.io/qml-challenge-scorer/ionqvision-docs/modules.html#ionqvision.modules.binary_classifier.BinaryMNISTClassifier.save_model" rel="noreferrer">documentation</a> of the optional <code>path_to_repo</code> parameter to work in a different directory.</p></div></aside></div><div id="nGI6JRvyMa" class="relative group/block article-grid subgrid-gap col-screen"><div class="flex sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:hidden"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="hidden sticky top-[80px] z-10 opacity-70 group-hover/block:opacity-100 group-hover/block:flex"><div class="absolute top-0 -right-[28px] flex md:flex-col"></div></div><div class="relative group not-prose overflow-auto shadow hover:shadow-md dark:shadow-2xl dark:shadow-neutral-900 my-5 text-sm border border-l-4 border-l-blue-400 border-gray-200 dark:border-l-blue-400 dark:border-gray-800"><pre style="display:block;overflow-x:auto;padding:0.8rem;background:transparent;color:black"><code class="language-python" style="white-space:pre">classifier.submit_model_for_grading()</code></pre><button title="Copy to Clipboard" class="inline-flex items-center opacity-0 group-hover:opacity-100 hover:opacity-100 focus:opacity-100 active:opacity-100 cursor-pointer ml-2 transition-color duration-200 ease-in-out text-blue-400 hover:text-blue-500 absolute right-1 top-1" aria-pressed="false" aria-label="Copy code to clipboard"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" width="24" height="24"><path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 01-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 011.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 00-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 01-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 00-3.375-3.375h-1.5a1.125 1.125 0 01-1.125-1.125v-1.5a3.375 3.375 0 00-3.375-3.375H9.75"></path></svg></button></div><div data-mdast-node-id="B2VLq5f686kMH_vO4Jukj" class="max-w-full overflow-y-visible overflow-x-auto m-0 group not-prose relative text-left mb-5"><div><pre class="text-sm font-thin font-system"><code><span>Success! Submitted trained_models/model_e5e7ca05465cd7b5661959c722b92a7ac291834d.zip for grading!
</span></code></pre></div></div></div><div id="nZ1i0TA4iZ" class="relative group/block article-grid subgrid-gap col-screen"><aside class="my-5 shadow-md dark:shadow-2xl dark:shadow-neutral-900 bg-gray-50/10 dark:bg-stone-800 overflow-hidden rounded border-l-4 border-blue-500"><div class="m-0 font-medium py-1 flex min-w-0 text-lg text-blue-600 bg-blue-50 dark:bg-slate-900"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" width="2rem" height="2rem" class="inline-block pl-2 mr-2 self-center flex-none text-blue-600"><path stroke-linecap="round" stroke-linejoin="round" d="M11.25 11.25l.041-.02a.75.75 0 011.063.852l-.708 2.836a.75.75 0 001.063.853l.041-.021M21 12a9 9 0 11-18 0 9 9 0 0118 0zm-9-3.75h.008v.008H12V8.25z"></path></svg><div class="text-neutral-900 dark:text-white grow self-center overflow-hidden break-words">Note</div></div><div class="px-4 py-1"><p>The <code>submit_model_for_grading</code> method prepares an archive containing your trained model’s weights, and everything that’s needed to rebuild your quantum layer. The archive is stored in the <code>trained_models</code> directory in your copy of our repository.</p><aside class="my-5 shadow-md dark:shadow-2xl dark:shadow-neutral-900 bg-gray-50/10 dark:bg-stone-800 overflow-hidden rounded border-l-4 border-amber-600"><div class="m-0 font-medium py-1 flex min-w-0 text-lg text-amber-600 bg-amber-50 dark:bg-slate-900"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" width="2rem" height="2rem" class="inline-block pl-2 mr-2 self-center flex-none text-amber-600"><path stroke-linecap="round" stroke-linejoin="round" d="M12 9v3.75m9-.75a9 9 0 11-18 0 9 9 0 0118 0zm-9 3.75h.008v.008H12v-.008z"></path></svg><div class="text-neutral-900 dark:text-white grow self-center overflow-hidden break-words">Caution</div></div><div class="px-4 py-1"><p>Don’t change the archive’s name or its contents.</p></div></aside></div></aside><p>Good luck now! We hope you enjoy this exciting challenge.</p></div><div class="flex pt-10 mb-10 space-x-4"><a class="flex-1 block p-4 font-normal text-gray-600 no-underline border border-gray-200 rounded shadow-sm group hover:border-blue-600 dark:hover:border-blue-400 hover:text-blue-600 dark:hover:text-blue-400 dark:text-gray-100 dark:border-gray-500 hover:shadow-lg dark:shadow-neutral-700" href="/qml-challenge-scorer/"><div class="flex h-full align-middle"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" width="1.5rem" height="1.5rem" class="self-center transition-transform group-hover:-translate-x-1 shrink-0"><path stroke-linecap="round" stroke-linejoin="round" d="M10.5 19.5L3 12m0 0l7.5-7.5M3 12h18"></path></svg><div class="flex-grow text-right"><div class="text-xs text-gray-500 dark:text-gray-400">Zero or one?</div>Zero or one?</div></div></a><a class="flex-1 block p-4 font-normal text-gray-600 no-underline border border-gray-200 rounded shadow-sm group hover:border-blue-600 dark:hover:border-blue-400 hover:text-blue-600 dark:hover:text-blue-400 dark:text-gray-100 dark:border-gray-500 hover:shadow-lg dark:shadow-neutral-700" href="/qml-challenge-scorer/details"><div class="flex h-full align-middle"><div class="flex-grow"><div class="text-xs text-gray-500 dark:text-gray-400">Zero or one?</div>Tips, tricks, and potential gotchas</div><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" width="1.5rem" height="1.5rem" class="self-center transition-transform group-hover:translate-x-1 shrink-0"><path stroke-linecap="round" stroke-linejoin="round" d="M13.5 4.5L21 12m0 0l-7.5 7.5M21 12H3"></path></svg></div></a></div></main></article><script>((o,u)=>{if(!window.history.state||!window.history.state.key){let f=Math.random().toString(32).slice(2);window.history.replaceState({key:f},"")}try{let d=JSON.parse(sessionStorage.getItem(o)||"{}")[u||window.history.state.key];typeof d=="number"&&window.scrollTo(0,d)}catch(f){console.error(f),sessionStorage.removeItem(o)}})("positions", null)</script><link rel="modulepreload" href="/qml-challenge-scorer/build/entry.client-K6EYMIDZ.js"/><link rel="modulepreload" href="/qml-challenge-scorer/build/_shared/chunk-FBLNPC3V.js"/><link rel="modulepreload" href="/qml-challenge-scorer/build/_shared/chunk-XJN3BT5Q.js"/><link rel="modulepreload" href="/qml-challenge-scorer/build/_shared/chunk-2NH4LW52.js"/><link rel="modulepreload" href="/qml-challenge-scorer/build/_shared/chunk-3BBIM4HZ.js"/><link rel="modulepreload" href="/qml-challenge-scorer/build/_shared/chunk-YAIQ7LUU.js"/><link rel="modulepreload" href="/qml-challenge-scorer/build/_shared/chunk-HTHE5KDW.js"/><link rel="modulepreload" href="/qml-challenge-scorer/build/_shared/chunk-AAPZCWUN.js"/><link rel="modulepreload" href="/qml-challenge-scorer/build/_shared/chunk-5JCL4YP4.js"/><link rel="modulepreload" href="/qml-challenge-scorer/build/_shared/chunk-QB27XQSF.js"/><link rel="modulepreload" href="/qml-challenge-scorer/build/_shared/chunk-JOLZ6LZB.js"/><link rel="modulepreload" href="/qml-challenge-scorer/build/_shared/chunk-MITWS4PZ.js"/><link rel="modulepreload" href="/qml-challenge-scorer/build/_shared/chunk-J6FHCSRC.js"/><link rel="modulepreload" href="/qml-challenge-scorer/build/_shared/chunk-EEG6LHX7.js"/><link rel="modulepreload" href="/qml-challenge-scorer/build/_shared/chunk-GUCIBHGO.js"/><link rel="modulepreload" href="/qml-challenge-scorer/build/root-3WANWRXC.js"/><link rel="modulepreload" href="/qml-challenge-scorer/build/_shared/chunk-A52HLF33.js"/><link rel="modulepreload" href="/qml-challenge-scorer/build/routes/$-CZ6MXPDJ.js"/><script>window.__remixContext = {"url":"/challenge-description","state":{"loaderData":{"root":{"theme":"light","config":{"options":{"favicon":"/qml-challenge-scorer/build/logo-4ab06c81ad74c531b0bcbc2fa2eb5a90.png","logo":"/qml-challenge-scorer/build/logo-4ab06c81ad74c531b0bcbc2fa2eb5a90.png","logo_dark":"/qml-challenge-scorer/build/logo_dark-70575cd31030c6b7d14d0524bad7aeaf.png"},"myst":"1.2.9","nav":[],"actions":[{"title":"IonQ Vision Docs","url":"https://willieab.github.io/qml-challenge-scorer/ionqvision-docs","internal":false,"static":false},{"title":"IonQ Cloud","url":"https://cloud.ionq.com","internal":false,"static":false}],"projects":[{"subject":"Quantum Vision Challenge","exports":[],"title":"Zero or one?","description":"An image classification challenge using hybrid quantum-classical neural networks","banner":"/qml-challenge-scorer/build/forte_header-caf18de3d14967ab2dea6f0808cc24cd.png","authors":[{"id":"Willie Aboumrad","name":"Willie Aboumrad"},{"id":"Sang Kim","name":"Sang Kim"}],"venue":{"title":"IonQ-SKKU Hackathon"},"github":"https://github.com/willieab/ionq-skku-vision-challenge","copyright":"IonQ, Inc (2024)","bibliography":[],"index":"index","pages":[{"slug":"challenge-description","title":"Your mission","description":"","date":"","thumbnail":"/qml-challenge-scorer/build/hybrid_nn-02bee137d9c820208ac883f635840f61.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"details","title":"Tips, tricks, and potential gotchas","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"env-setup","title":"Environment set up","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"leaderboard","title":"Leader board","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1}]}]},"CONTENT_CDN_PORT":"3100","MODE":"static","BASE_URL":"/qml-challenge-scorer"},"routes/$":{"config":{"options":{"favicon":"/qml-challenge-scorer/build/logo-4ab06c81ad74c531b0bcbc2fa2eb5a90.png","logo":"/qml-challenge-scorer/build/logo-4ab06c81ad74c531b0bcbc2fa2eb5a90.png","logo_dark":"/qml-challenge-scorer/build/logo_dark-70575cd31030c6b7d14d0524bad7aeaf.png"},"myst":"1.2.9","nav":[],"actions":[{"title":"IonQ Vision Docs","url":"https://willieab.github.io/qml-challenge-scorer/ionqvision-docs","internal":false,"static":false},{"title":"IonQ Cloud","url":"https://cloud.ionq.com","internal":false,"static":false}],"projects":[{"subject":"Quantum Vision Challenge","exports":[],"title":"Zero or one?","description":"An image classification challenge using hybrid quantum-classical neural networks","banner":"/qml-challenge-scorer/build/forte_header-caf18de3d14967ab2dea6f0808cc24cd.png","authors":[{"id":"Willie Aboumrad","name":"Willie Aboumrad"},{"id":"Sang Kim","name":"Sang Kim"}],"venue":{"title":"IonQ-SKKU Hackathon"},"github":"https://github.com/willieab/ionq-skku-vision-challenge","copyright":"IonQ, Inc (2024)","bibliography":[],"index":"index","pages":[{"slug":"challenge-description","title":"Your mission","description":"","date":"","thumbnail":"/qml-challenge-scorer/build/hybrid_nn-02bee137d9c820208ac883f635840f61.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"details","title":"Tips, tricks, and potential gotchas","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"env-setup","title":"Environment set up","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"leaderboard","title":"Leader board","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1}]}]},"page":{"kind":"Notebook","sha256":"95608085e3f216e9e1ecb4e33d7e123c8705e98c42a3791a02a4277e55db09ff","slug":"challenge-description","location":"/challenge_description.ipynb","dependencies":[],"frontmatter":{"title":"Your mission","authors":[{"id":"Willie Aboumrad","name":"Willie Aboumrad"}],"kernelspec":{"name":"python3","language":"python","display_name":"Python 3 (ipykernel)"},"github":"https://github.com/willieab/ionq-skku-vision-challenge","subject":"Quantum Vision Challenge","venue":{"title":"IonQ-SKKU Hackathon"},"copyright":"IonQ, Inc (2024)","thumbnail":"/qml-challenge-scorer/build/hybrid_nn-02bee137d9c820208ac883f635840f61.png","exports":[{"format":"ipynb","filename":"challenge_description.ipynb","url":"/qml-challenge-scorer/build/challenge_descriptio-80e8498cebe08037aaabe3c8aba772dd.ipynb"}]},"mdast":{"type":"root","children":[{"type":"block","kind":"notebook-content","data":{},"children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"In this challenge you’ll implement a hybrid quantum-classical image classification pipeline using ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ynVdF8w3K8"},{"type":"inlineCode","value":"ionqvision","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"bWIQwvgxE0"},{"type":"text","value":", IonQ’s next-generation platform for QML computations, and ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"lnlhx2LLjr"},{"type":"link","url":"https://pytorch.org","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"inlineCode","value":"PyTorch","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"fRCG7d3PGT"}],"urlSource":"https://pytorch.org","key":"BA0HGOEiDh"},{"type":"text","value":". In particular, you’ll train a quantum-classical neural network to distinguish between two types of different handwritten digits in the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"xg5HAlKD80"},{"type":"link","url":"https://en.wikipedia.org/wiki/MNIST_database","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"MNIST database","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"fQzNYgKMg2"}],"urlSource":"wiki:MNIST_database","data":{"page":"MNIST_database","wiki":"https://en.wikipedia.org/","lang":"en"},"internal":false,"protocol":"wiki","key":"MVBcziJJ5f"},{"type":"text","value":".","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"LRflSpfwN1"}],"key":"pVz4ypKW3G"},{"type":"admonition","kind":"hint","children":[{"type":"admonitionTitle","children":[{"type":"text","value":"Tip","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"TkWf5ngOdy"}],"key":"Ia7fFfg62A"},{"type":"paragraph","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"If you haven’t used ","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"m08wEQHV7o"},{"type":"inlineCode","value":"PyTorch","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"ba4a4V3krn"},{"type":"text","value":" yet, don’t sweat it!","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"L32JDR2INR"}],"key":"ZHsXIy4AVR"},{"type":"paragraph","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"text","value":"This challenge is about the ","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"biM4jwc2Ml"},{"type":"strong","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"text","value":"quantum","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"Ct23GzSUhW"}],"key":"O8jwZWd1hr"},{"type":"text","value":" in ","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"YLyacdDL1K"},{"type":"link","url":"https://en.wikipedia.org/wiki/Quantum_machine_learning","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"text","value":"“Quantum Machine Learning”","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"zXdjaKL29L"}],"urlSource":"wiki:Quantum_machine_learning","data":{"page":"Quantum_machine_learning","wiki":"https://en.wikipedia.org/","lang":"en"},"internal":false,"protocol":"wiki","key":"iOFsO5za88"},{"type":"text","value":". The accompanying ","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"xGV1zQ33z5"},{"type":"crossReference","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"text","value":"starter code","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"YRslTvSmCB"}],"identifier":"sec-training","label":"sec-training","kind":"heading","template":"{name}","resolved":true,"html_id":"sec-training","key":"TL66LI2o9c"},{"type":"text","value":" will set up everything you need, so you can focus on building your best quantum circuits!","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"J4ywvx1awx"}],"key":"UajEeFGC6A"}],"key":"urSISFH4yh"},{"type":"paragraph","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"text","value":"The figure below illustrates our hybrid pipeline. The operations inside the gray box make up the ","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"wsGCdV8eHu"},{"type":"emphasis","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"text","value":"quantum layer","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"u8bX9Fzg1K"}],"key":"BE4n2jQSmP"},{"type":"text","value":". That’s where the magic happens.","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"hRErhSVdri"}],"key":"jdj9UcJl0z"},{"type":"container","kind":"figure","identifier":"fig-hybrid-nn","label":"fig-hybrid-nn","children":[{"type":"image","url":"/qml-challenge-scorer/build/hybrid_nn-02bee137d9c820208ac883f635840f61.png","alt":"A diagrammatic representation of our hybrid quantum-classical image classification pipeline","data":{"altTextIsAutoGenerated":true},"key":"MCpiIo2EEc","urlSource":"media/hybrid_nn.png"},{"type":"caption","children":[{"type":"paragraph","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"children":[{"type":"captionNumber","kind":"figure","label":"fig-hybrid-nn","identifier":"fig-hybrid-nn","html_id":"fig-hybrid-nn","enumerator":"1","children":[{"type":"text","value":"Figure ","key":"tNjlvUM92L"},{"type":"text","value":"1","key":"LY8oVOOuzc"},{"type":"text","value":":","key":"OUCAkSpC1P"}],"template":"Figure %s:","key":"fWHEiZqkYa"},{"type":"text","value":"A diagrammatic representation of our hybrid quantum-classical image classification pipeline","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"key":"lydqEh8qiz"}],"key":"B7CH1fsgWR"}],"key":"UYdAl5ZQ4F"}],"enumerator":"1","html_id":"fig-hybrid-nn","key":"dSCcAVujSe"},{"type":"paragraph","position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"children":[{"type":"text","value":"Your mission, should you choose to accept it, is to design the quantum layer that yields the ","position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"key":"Vfav3e2B5E"},{"type":"strong","position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"children":[{"type":"text","value":"highest possible classification accuracy","position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"key":"bNfe6jVpik"}],"key":"tnjDgcQmCV"},{"type":"text","value":" (scored automatically on an unseen validation set). Simple, right?","position":{"start":{"line":17,"column":1},"end":{"line":17,"column":1}},"key":"UOlVAxIcnD"}],"key":"Cv3fRtykO5"},{"type":"heading","depth":2,"position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"children":[{"type":"text","value":"Classical, quantum, and then classical again","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"key":"mSSGzVZOFW"}],"identifier":"classical-quantum-and-then-classical-again","label":"Classical, quantum, and then classical again","html_id":"classical-quantum-and-then-classical-again","implicit":true,"key":"RyedFgeGSG"},{"type":"paragraph","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"children":[{"type":"text","value":"If you’ve made it thus far, congratulations! 🎉","position":{"start":{"line":21,"column":1},"end":{"line":21,"column":1}},"key":"lxjIZsEMHi"}],"key":"SOALnWWPbu"},{"type":"paragraph","position":{"start":{"line":23,"column":1},"end":{"line":23,"column":1}},"children":[{"type":"text","value":"You’re about to embark on an exciting journey of quantum discovery. To begin, we’ll take a closer look at the architecture of our hybrid neural network. It’s important to keep in mind that going through our full ","position":{"start":{"line":23,"column":1},"end":{"line":23,"column":1}},"key":"ilvrPY4zpL"},{"type":"emphasis","position":{"start":{"line":23,"column":1},"end":{"line":23,"column":1}},"children":[{"type":"text","value":"hybrid","position":{"start":{"line":23,"column":1},"end":{"line":23,"column":1}},"key":"bkKLv22A9F"}],"key":"dzfC0NipME"},{"type":"text","value":" pipeline requires ","position":{"start":{"line":23,"column":1},"end":{"line":23,"column":1}},"key":"VoEfdYPOGd"},{"type":"strong","position":{"start":{"line":23,"column":1},"end":{"line":23,"column":1}},"children":[{"type":"text","value":"both","position":{"start":{"line":23,"column":1},"end":{"line":23,"column":1}},"key":"Hyoe2A6b5I"}],"key":"uwi4XyNn7K"},{"type":"text","value":" classical and quantum hardware, and software that facilitates communication between the two. The ","position":{"start":{"line":23,"column":1},"end":{"line":23,"column":1}},"key":"N23S9GbiW6"},{"type":"inlineCode","value":"ionqvision","position":{"start":{"line":23,"column":1},"end":{"line":23,"column":1}},"key":"S3ZSMnxKKo"},{"type":"text","value":" abstracts much of this complexity away and it let’s you focus on the quantum design.","position":{"start":{"line":23,"column":1},"end":{"line":23,"column":1}},"key":"yekuheCEe7"}],"key":"k1TDosnoKf"},{"type":"paragraph","position":{"start":{"line":25,"column":1},"end":{"line":25,"column":1}},"children":[{"type":"text","value":"In what follows, we’ll explain the classical and quantum parts of our pipeline in some detail. Then we’ll turn to the ","position":{"start":{"line":25,"column":1},"end":{"line":25,"column":1}},"key":"yibNJB70r5"},{"type":"crossReference","position":{"start":{"line":25,"column":1},"end":{"line":25,"column":1}},"children":[{"type":"text","value":"starter code","position":{"start":{"line":25,"column":1},"end":{"line":25,"column":1}},"key":"f24vGroZ2C"}],"identifier":"sec-training","label":"sec-training","kind":"heading","template":"{name}","resolved":true,"html_id":"sec-training","key":"lSAkp6D56A"},{"type":"text","value":", which will get you up and running in no time!","position":{"start":{"line":25,"column":1},"end":{"line":25,"column":1}},"key":"clPbcd5yGR"}],"key":"xeNEmCGkxE"},{"type":"heading","depth":3,"position":{"start":{"line":27,"column":1},"end":{"line":27,"column":1}},"children":[{"type":"text","value":"Classical pre-processing","position":{"start":{"line":27,"column":1},"end":{"line":27,"column":1}},"key":"QgrO1SdwwP"}],"identifier":"classical-pre-processing","label":"Classical pre-processing","html_id":"classical-pre-processing","implicit":true,"key":"Cehayy5CEr"},{"type":"paragraph","position":{"start":{"line":29,"column":1},"end":{"line":29,"column":1}},"children":[{"type":"text","value":"The first three layers in our pipeline are purely classical. They implement fairly basic operations commonly encountered in classical image classification pipelines: first flatten each (single-channel) input image, perform dimensionality reduction using ","position":{"start":{"line":29,"column":1},"end":{"line":29,"column":1}},"key":"LZD6724ut4"},{"type":"link","url":"https://en.wikipedia.org/wiki/Principal_component_analysis","position":{"start":{"line":29,"column":1},"end":{"line":29,"column":1}},"children":[{"type":"text","value":"PCA","position":{"start":{"line":29,"column":1},"end":{"line":29,"column":1}},"key":"aiQexXQA4D"}],"urlSource":"wiki:Principal_component_analysis","data":{"page":"Principal_component_analysis","wiki":"https://en.wikipedia.org/","lang":"en"},"internal":false,"protocol":"wiki","key":"LqcuNw5UFA"},{"type":"text","value":", and then embed the compressed images into an even lower-dimensional feature space using a trainable fully-connected layer. We use drop out during training and the sigmoid ","position":{"start":{"line":29,"column":1},"end":{"line":29,"column":1}},"key":"QEq4GAQNdJ"},{"type":"link","url":"https://en.wikipedia.org/wiki/Activation_function","position":{"start":{"line":29,"column":1},"end":{"line":29,"column":1}},"children":[{"type":"text","value":"activation function","position":{"start":{"line":29,"column":1},"end":{"line":29,"column":1}},"key":"tVk8fmrbpT"}],"urlSource":"https://en.wikipedia.org/wiki/Activation_function","data":{"page":"Activation_function","wiki":"https://en.wikipedia.org/","lang":"en"},"internal":false,"protocol":"wiki","key":"FuqJyVZS2k"},{"type":"text","value":".","position":{"start":{"line":29,"column":1},"end":{"line":29,"column":1}},"key":"MXXvn2dMo1"}],"key":"nMCqmY6PMq"},{"type":"image","url":"/qml-challenge-scorer/build/classical_pre-96198977b57d82ee819e306155372f1c.png","width":"75%","align":"center","key":"WzCp2dCRbV","urlSource":"media/classical_pre.png"},{"type":"admonition","kind":"warning","children":[{"type":"admonitionTitle","children":[{"type":"text","value":"Note","position":{"start":{"line":35,"column":1},"end":{"line":35,"column":1}},"key":"v8rCM2anjN"}],"key":"RK9oX67dMR"},{"type":"paragraph","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"children":[{"type":"text","value":"This part of the architecture is essentially ","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"key":"yf5vKKUIT9"},{"type":"strong","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"children":[{"type":"text","value":"fixed","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"key":"pKpWdeWPU6"}],"key":"p4MhrHlvmN"},{"type":"text","value":": the only thing you get to decide here is the dimension ","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"key":"qZO2ID8i3u"},{"type":"inlineMath","value":"n","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"html":"\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003en\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003en\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.4306em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003en\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e","key":"U0RmC2sESX"},{"type":"text","value":" of the resulting latent vectors. For instance, in ","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"key":"KbxBwA0wf9"},{"type":"crossReference","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"children":[{"type":"text","value":"Figure ","key":"klxGYuSKjS"},{"type":"text","value":"1","key":"w9l58JPsE3"}],"identifier":"fig-hybrid-nn","label":"fig-hybrid-nn","kind":"figure","template":"Figure %s","enumerator":"1","resolved":true,"html_id":"fig-hybrid-nn","key":"Bz6tXwtrQG"},{"type":"text","value":", ","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"key":"qIIOKa6e26"},{"type":"inlineMath","value":"n = 12","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"html":"\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003en\u003c/mi\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmn\u003e12\u003c/mn\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003en = 12\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.4306em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003en\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003e\u003c/span\u003e\u003cspan class=\"mrel\"\u003e=\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.6444em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e12\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e","key":"w8JbLx8WYo"},{"type":"text","value":".","position":{"start":{"line":37,"column":1},"end":{"line":37,"column":1}},"key":"VDxRLQtfB3"}],"key":"q93DnZiQkq"}],"key":"cDcOLZQq4r"},{"type":"heading","depth":3,"position":{"start":{"line":41,"column":1},"end":{"line":41,"column":1}},"children":[{"type":"text","value":"Quantum Layer","position":{"start":{"line":41,"column":1},"end":{"line":41,"column":1}},"key":"lk2NkPaERC"}],"identifier":"sec-quantum-layer","label":"sec-quantum-layer","html_id":"sec-quantum-layer","key":"za3crgOJcR"},{"type":"paragraph","position":{"start":{"line":43,"column":1},"end":{"line":43,"column":1}},"children":[{"type":"text","value":"Then comes the most interesting part: the “quantum layer.” Our whole challenge revolves around this layer: the best design here will achieve the highest classification accuracy. And there’s a ","position":{"start":{"line":43,"column":1},"end":{"line":43,"column":1}},"key":"k9tAp0EvTb"},{"type":"strong","position":{"start":{"line":43,"column":1},"end":{"line":43,"column":1}},"children":[{"type":"text","value":"big prize","position":{"start":{"line":43,"column":1},"end":{"line":43,"column":1}},"key":"pTSPz2Nyx9"}],"key":"kzL5BnTzen"},{"type":"text","value":" for that. 🥇","position":{"start":{"line":43,"column":1},"end":{"line":43,"column":1}},"key":"FJMTiTGCJE"}],"key":"ZbHxlL8E8G"},{"type":"paragraph","position":{"start":{"line":45,"column":1},"end":{"line":45,"column":1}},"children":[{"type":"text","value":"The quantum layer consists of three components:","position":{"start":{"line":45,"column":1},"end":{"line":45,"column":1}},"key":"KMc5TA6C9h"}],"key":"q6vRvYnETU"},{"type":"list","ordered":true,"start":1,"spread":false,"position":{"start":{"line":47,"column":1},"end":{"line":50,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"children":[{"type":"text","value":"an encoding circuit used to load each ","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"key":"FiGDsOjbHk"},{"type":"inlineMath","value":"n","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"html":"\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003en\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003en\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.4306em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003en\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e","key":"hVNusNhSt5"},{"type":"text","value":"-dimensional latent vector into an ","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"key":"aUITXrpLIB"},{"type":"inlineMath","value":"n","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"html":"\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003en\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003en\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.4306em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003en\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e","key":"I4S15WWaP6"},{"type":"text","value":"-dimensional qubit state,","position":{"start":{"line":47,"column":1},"end":{"line":47,"column":1}},"key":"u1itiaYyXu"}],"key":"u3gUjwvinn"},{"type":"listItem","spread":true,"position":{"start":{"line":48,"column":1},"end":{"line":48,"column":1}},"children":[{"type":"text","value":"a variational circuit on ","position":{"start":{"line":48,"column":1},"end":{"line":48,"column":1}},"key":"pBiOEoL30D"},{"type":"inlineMath","value":"n","position":{"start":{"line":48,"column":1},"end":{"line":48,"column":1}},"html":"\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003en\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003en\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.4306em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003en\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e","key":"QzYeAI9P71"},{"type":"text","value":" qubits with trainable parameters used to learn how to separate encoded latent vectors in the high-dimensional multi-qubit state space, and","position":{"start":{"line":48,"column":1},"end":{"line":48,"column":1}},"key":"nVRtBbeNia"}],"key":"soGJeLCaBR"},{"type":"listItem","spread":true,"position":{"start":{"line":49,"column":1},"end":{"line":50,"column":1}},"children":[{"type":"text","value":"a list of ","position":{"start":{"line":49,"column":1},"end":{"line":49,"column":1}},"key":"dppy3Ka93m"},{"type":"inlineMath","value":"m","position":{"start":{"line":49,"column":1},"end":{"line":49,"column":1}},"html":"\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003em\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003em\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.4306em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003em\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e","key":"mJc3c2fbGk"},{"type":"text","value":" measureable quantities used to extract “quantum features” from the transformed ","position":{"start":{"line":49,"column":1},"end":{"line":49,"column":1}},"key":"hTYYODKAkI"},{"type":"inlineMath","value":"n","position":{"start":{"line":49,"column":1},"end":{"line":49,"column":1}},"html":"\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003en\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003en\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.4306em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003en\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e","key":"QfG5x53xbO"},{"type":"text","value":"-qubit states for classification. (For instance, ","position":{"start":{"line":49,"column":1},"end":{"line":49,"column":1}},"key":"eS9IZwhWYf"},{"type":"inlineMath","value":"m = 4","position":{"start":{"line":49,"column":1},"end":{"line":49,"column":1}},"html":"\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003em\u003c/mi\u003e\u003cmo\u003e=\u003c/mo\u003e\u003cmn\u003e4\u003c/mn\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003em = 4\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.4306em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003em\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003e\u003c/span\u003e\u003cspan class=\"mrel\"\u003e=\u003c/span\u003e\u003cspan class=\"mspace\" style=\"margin-right:0.2778em;\"\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.6444em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord\"\u003e4\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e","key":"xMssVP00yP"},{"type":"text","value":" in ","position":{"start":{"line":49,"column":1},"end":{"line":49,"column":1}},"key":"XL8YmqlPT1"},{"type":"crossReference","position":{"start":{"line":49,"column":1},"end":{"line":49,"column":1}},"children":[{"type":"text","value":"Figure ","key":"BrozDRxY9W"},{"type":"text","value":"1","key":"WmAn1bVX7R"}],"identifier":"fig-hybrid-nn","label":"fig-hybrid-nn","kind":"figure","template":"Figure %s","enumerator":"1","resolved":true,"html_id":"fig-hybrid-nn","key":"vPhh8qW2IK"},{"type":"text","value":".)","position":{"start":{"line":49,"column":1},"end":{"line":49,"column":1}},"key":"OktJDKAhgo"}],"key":"VsLtLa2Hdc"}],"key":"BlevECXVcl"},{"type":"image","url":"/qml-challenge-scorer/build/quantum_layer-798e7871a9d7e580cdba7345274c5ce4.png","width":"75%","align":"center","key":"td2rQd0KSj","urlSource":"media/quantum_layer.png"},{"type":"paragraph","position":{"start":{"line":55,"column":1},"end":{"line":55,"column":1}},"children":[{"type":"text","value":"To get the ball rolling, look to the ","position":{"start":{"line":55,"column":1},"end":{"line":55,"column":1}},"key":"MGG7eDLmo5"},{"type":"inlineCode","value":"ionqvision.ansatze.ansatz_library","position":{"start":{"line":55,"column":1},"end":{"line":55,"column":1}},"key":"C7wzbVT75p"},{"type":"text","value":" module for inpiration. We’ve implemented some of the encoders and ansatze commonly encountered in the literature.","position":{"start":{"line":55,"column":1},"end":{"line":55,"column":1}},"key":"TTiDBbLPCi"}],"key":"cOrcUHvAtV"},{"type":"paragraph","position":{"start":{"line":57,"column":1},"end":{"line":57,"column":1}},"children":[{"type":"text","value":"For instance, you could use the ","position":{"start":{"line":57,"column":1},"end":{"line":57,"column":1}},"key":"fhMoqnw71e"},{"type":"inlineCode","value":"AngleEncoder","position":{"start":{"line":57,"column":1},"end":{"line":57,"column":1}},"key":"KZPrBfwP8N"},{"type":"text","value":", which has the following structure, for your first model.","position":{"start":{"line":57,"column":1},"end":{"line":57,"column":1}},"key":"vmRlVlyLHc"}],"key":"Kj7hK9hEgx"}],"key":"vl9fXwmDUA"},{"type":"block","kind":"notebook-code","data":{},"children":[{"type":"code","lang":"python","executable":true,"value":"from ionqvision.ansatze.ansatz_library import AngleEncoder\n\nencoder = AngleEncoder(num_qubits=4)\nencoder.draw(\"mpl\")","key":"QpyKrOHtYI"},{"type":"output","id":"PAwV1KcfRy4yisjlNhpBu","data":[{"output_type":"execute_result","execution_count":1,"metadata":{},"data":{"image/png":{"content_type":"image/png","hash":"6f193422c8d03a43328e34cf4782af51","path":"/qml-challenge-scorer/build/6f193422c8d03a43328e34cf4782af51.png"},"text/plain":{"content":"\u003cFigure size 454.719x367.889 with 1 Axes\u003e","content_type":"text/plain"}}}],"key":"uEVKCeXQQg"}],"key":"uj3clRVymo"},{"type":"block","kind":"notebook-content","data":{},"children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"In addition, you can use its implementation as a ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"TMQ6jfNAfx"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"template","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"jIBbipebrm"}],"key":"QwCOZjt8Bj"},{"type":"text","value":" for your novel encoder designs!","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Aq7yUkrleF"}],"key":"Kljy7YVkna"},{"type":"include","file":"../ionqvision/ionqvision/ansatze/ansatz_library.py","literal":true,"filter":{"lines":[[19,57]]},"children":[{"type":"code","value":"class AngleEncoder(VariationalAnsatz):\n    \"\"\"\n    Implement a quantum circuit for higher-order sparse angle encoding.\n\n    INPUT:\n\n        - ``num_qubits`` -- number of qubits\n        - ``entanglement_depth`` -- (optional) number layers of entangling CNOT\n          gates: for each ``k`` in ``range(entanglement_depth)``, use gates\n          ``CNOT(j, j + k + 1)``.\n        - ``param_prefix`` -- (optional) string prefix for named circuit\n          parameters\n\n    EXAMPLES::\n\n        \u003e\u003e\u003e from ionqvision.ansatze.ansatz_library import AngleEncoder\n        \u003e\u003e\u003e ansatz = AngleEncoder(4, entanglement_depth=3, param_prefix=\"y\")\n        \u003e\u003e\u003e ansatz.draw()\n             ┌────────────┐                              \n        q_0: ┤ Ry(π*y[0]) ├──■──────────────■─────────■──\n             ├────────────┤┌─┴─┐            │         │  \n        q_1: ┤ Ry(π*y[1]) ├┤ X ├──■─────────┼────■────┼──\n             ├────────────┤└───┘┌─┴─┐     ┌─┴─┐  │    │  \n        q_2: ┤ Ry(π*y[2]) ├─────┤ X ├──■──┤ X ├──┼────┼──\n             ├────────────┤     └───┘┌─┴─┐└───┘┌─┴─┐┌─┴─┐\n        q_3: ┤ Ry(π*y[3]) ├──────────┤ X ├─────┤ X ├┤ X ├\n             └────────────┘          └───┘     └───┘└───┘\n    \"\"\"\n    def __init__(self, num_qubits, entanglement_depth=1, param_prefix=\"x\"):\n        super().__init__(num_qubits)\n\n        x = ParameterVector(param_prefix, num_qubits)\n        [self.ry(np.pi * xi, qbt) for qbt, xi in enumerate(x)]\n\n        for k in range(entanglement_depth):\n            top_qubit = 0\n            while top_qubit \u003c num_qubits - (k + 1):\n                self.cx(top_qubit, top_qubit + k + 1)\n                top_qubit += 1","lang":"python","showLineNumbers":true,"startingLineNumber":19,"filename":"ansatz_library.py","key":"x7KXY87gNC"}],"key":"Mb0DWjLF4w"},{"type":"paragraph","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"text","value":"Similarly, you can leverage the built-in ","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"u1J3LShYJs"},{"type":"inlineCode","value":"BrickworkLayoutAnsatz","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"jkLM1UZ77U"},{"type":"text","value":", or the ","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"YXBoq7eps1"},{"type":"inlineCode","value":"QCNNAnsatz","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"PnZY1ofZSe"},{"type":"text","value":", amongst others, for the trainable layer.","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"H92PVHbGEu"}],"key":"rlBLqAxVEC"}],"key":"slaSkoLr94"},{"type":"block","kind":"notebook-code","data":{},"children":[{"type":"code","lang":"python","executable":true,"value":"from ionqvision.ansatze.ansatz_library import QCNNAnsatz\n\nansatz = QCNNAnsatz(num_qubits=4)\nansatz.draw(\"mpl\")","key":"hhMFJCeVUW"},{"type":"output","id":"9P3QdrUGkEPCBWmwVcAdJ","data":[{"output_type":"execute_result","execution_count":2,"metadata":{},"data":{"image/png":{"content_type":"image/png","hash":"2b2c08850ab12a480f4662f2618dec5a","path":"/qml-challenge-scorer/build/2b2c08850ab12a480f4662f2618dec5a.png"},"text/plain":{"content":"\u003cFigure size 1207.22x367.889 with 1 Axes\u003e","content_type":"text/plain"}}}],"key":"M0vCs67tQB"}],"key":"GW0kjF5Tom"},{"type":"block","kind":"notebook-content","data":{},"children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"This ansatz is particularly interesting as it consists of a sequence of “convolution” filters interspersed with pooling operations that reduce the number of “active” qubits upon each layer. Note that our implemenation modifies the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"xRVISNqoS9"},{"type":"link","url":"https://www.nature.com/articles/s41567-019-0648-8","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"original design","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Sy0aVroTvE"}],"urlSource":"https://www.nature.com/articles/s41567-019-0648-8","key":"iczyNnCExK"},{"type":"text","value":" by replacing the mid-circuit measurements with controlled rotations.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"qt3YcMyX7H"}],"key":"CgI5BTCIai"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"If you’d like to try out this ansatz, be sure to ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"KcltsP7xSv"},{"type":"link","url":"https://pybit.es/articles/python-subclasses/","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"subclass","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"k3wcWw0SMx"}],"urlSource":"https://pybit.es/articles/python-subclasses/","key":"DWZYDO8T8H"},{"type":"text","value":" and implement your own ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"CIc22pK8lc"},{"type":"inlineCode","value":"QCNNAnsatz.ConvolutionBrickwork","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"iiHp3VbITb"},{"type":"text","value":" and ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"SEK1v5sAO9"},{"type":"inlineCode","value":"QCNN.PoolingLayer","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"aE5yRwC7CO"},{"type":"text","value":" modules!","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"R0p83Vtlkd"}],"key":"ERy4IsKRwU"},{"type":"include","file":"../ionqvision/ionqvision/ansatze/ansatz_library.py","literal":true,"filter":{"lines":[[271,330]]},"children":[{"type":"code","value":"class QCNNAnsatz(VariationalAnsatz):\n    r\"\"\"\n    Implement the Quantum Convolutional Network Ansatz (QCNN) as described in\n    :cite:t:`2019:qcnn`.\n\n    The quasi-local unitary $U_i$'s are entangling two-qubit gates with $6$\n    variational parameters.\n    They are laid out in a brickwork pattern with ``filter_depth`` layers.\n\n    The pooling operations are implemented by two-qubit controlled rotations,\n    with $2$ variational parameters.\n\n    The circuit starts with ``num_qubits`` active qubits and then half the\n    remaining qubits are discarded after each pooling operation until only a\n    single active qubit remains. This final qubit is measured and the result is\n    used for binary classification.\n    \"\"\"\n    class ConvolutionBrickwork(BrickworkLayoutAnsatz):\n        \"\"\"\n        Implement the convolution filters for the :class:`.QCNNAnsatz`.\n        \"\"\"\n        def __init__(self, num_qubits, num_layers, prefix=None, qubits=None, initial_state=None):\n            super().__init__(num_qubits, num_layers, blk_sz=3, prefix=prefix, qubits=qubits, initial_state=initial_state)\n        \n        def two_qubit_block(self, theta, q1, q2):\n            conv_op = QuantumCircuit(2, name=\"CONV\")\n            conv_op.ry(theta[0], 0)\n            conv_op.ry(theta[1], 1)\n            conv_op.rxx(theta[2], 0, 1)\n            self.append(conv_op.to_instruction(), [q1, q2])\n\n    class PoolingLayer(BrickworkLayoutAnsatz):\n        \"\"\"\n        Implement the pooling layer for the :class:`.QCNNAnsatz`.\n        \"\"\"\n        def __init__(self, num_qubits, prefix=None, qubits=None):\n            super().__init__(num_qubits, 1, blk_sz=1, prefix=prefix, qubits=qubits)\n    \n        def two_qubit_block(self, theta, q1, q2):\n            pool_op = QuantumCircuit(2, name=\"POOL\")\n            pool_op.crz(theta[0], 1, 0)\n            self.append(pool_op.to_instruction(), [q1, q2])\n\n    def __init__(self, num_qubits, filter_depth=2, initial_state=None):\n        num_layers = int(log(num_qubits, 2))\n        if abs(log(num_qubits, 2) - num_layers) \u003e 1e-6:\n            raise ValueError(\"num_qubits must be a power of 2\")\n\n        super().__init__(num_qubits)\n        if initial_state is not None:\n            self.compose(initial_state, inplace=True)\n\n        for k in range(num_layers):\n            qubits = list(range(0, num_qubits, 2**k))\n        \n            conv = QCNNAnsatz.ConvolutionBrickwork(num_qubits, filter_depth, prefix=\"C\" + str(k), qubits=qubits)\n            self.compose(conv, inplace=True)\n            \n            pool = QCNNAnsatz.PoolingLayer(num_qubits, prefix=\"P\" + str(k), qubits=qubits)\n            self.compose(pool, inplace=True)","lang":"python","showLineNumbers":true,"startingLineNumber":271,"filename":"ansatz_library.py","key":"YdzemWxPoU"}],"key":"oHMfsH9aNr"},{"type":"paragraph","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"text","value":"While ","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"yNb7iDH6BN"},{"type":"inlineCode","value":"ionqvision","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"HOlXXyqlfq"},{"type":"text","value":" does not provide built-in qubit observables, you can set these up using ","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"KmNyCoiCa2"},{"type":"inlineCode","value":"qiskit","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"zA2PPaPK7Y"},{"type":"text","value":" as follows.","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"Oc9mexG1ye"}],"key":"vXJWTbtFlp"}],"key":"xNY1bkmmeb"},{"type":"block","kind":"notebook-code","data":{},"children":[{"type":"code","lang":"python","executable":true,"value":"from qiskit.quantum_info import SparsePauliOp\n\n# Measure the expectation value of X_0, Y_0, Z_0\nquantum_features = [\n    SparsePauliOp([\"IIIX\"]), \n    SparsePauliOp([\"IIIY\"]), \n    SparsePauliOp([\"IIIZ\"])\n]","key":"wR0bPEPkfT"},{"type":"output","id":"sGlnzc61QCaSthAFrHu7u","data":[],"key":"vjciJSBiL7"}],"key":"rwhyvR8bpq"},{"type":"block","kind":"notebook-content","data":{},"children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"It’s important to keep in mind that the encoder, the ansatz, and the quantum feature vector are highly interrelated: the way you embed latent vectors into (multi-)qubit state space should dictate how you choose to transform the encoded state vectors, which should in turn inform what features you decide to measure.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"uDpuT7hzmU"}],"key":"VzPObgeotO"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"The best model will likely exploit synergies resulting from intentional co-design of the three components.","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"Ishygjgz3x"}],"key":"fU8GyJGEzF"},{"type":"admonition","kind":"important","children":[{"type":"admonitionTitle","children":[{"type":"text","value":"Important","key":"RklSnfYJOE"}],"key":"uQhQtBGxF9"},{"type":"paragraph","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"text","value":"This is where you should let your imagination run free! Get creative, and show us what you’ve got. Feel free to leverage the power of the internet and use every resource at your disposal.","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"s1m2LKXwn1"}],"key":"ItjVDvLhfe"}],"key":"JaoClX120O"},{"type":"heading","depth":3,"position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"text","value":"Classical post-processing","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"xFenkDfYda"}],"identifier":"classical-post-processing","label":"Classical post-processing","html_id":"classical-post-processing","implicit":true,"key":"CMErPAqiPi"},{"type":"paragraph","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"children":[{"type":"text","value":"After the quantum layer, feature vectors in the classification pipeline return to the classical device for some post-processing. In particular, we train a fully-connected layer with a scalar output to minimize the binary ","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"iky6GLsINs"},{"type":"link","url":"https://en.wikipedia.org/wiki/Cross-entropy","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"children":[{"type":"text","value":"cross-entropy","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"vtPUPq0uua"}],"urlSource":"wiki:Cross-entropy","data":{"page":"Cross-entropy","wiki":"https://en.wikipedia.org/","lang":"en"},"internal":false,"protocol":"wiki","key":"O65zUin8o7"},{"type":"text","value":" between the final value and the input image’s true label. Again, we use drop out during training and the sigmoid ","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"iOkrSISeei"},{"type":"link","url":"https://en.wikipedia.org/wiki/Activation_function","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"children":[{"type":"text","value":"activation function","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"qt1PE61Ll3"}],"urlSource":"https://en.wikipedia.org/wiki/Activation_function","data":{"page":"Activation_function","wiki":"https://en.wikipedia.org/","lang":"en"},"internal":false,"protocol":"wiki","key":"g90Z2D9YFI"},{"type":"text","value":".","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"Ng9dUUNEvK"}],"key":"omVUtDmvkG"},{"type":"image","url":"/qml-challenge-scorer/build/classical_post-af3006acd6306ca61c870ff9530707a4.png","width":"45%","align":"center","key":"ntA1vosyL2","urlSource":"media/classical_post.png"},{"type":"paragraph","position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"children":[{"type":"text","value":"This final stage is intentionally light, to ensure the quantum layer is the star of the show.","position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"key":"VfG3RfQvma"}],"key":"MDyyOIREAE"},{"type":"admonition","kind":"warning","children":[{"type":"admonitionTitle","children":[{"type":"text","value":"Note","position":{"start":{"line":20,"column":1},"end":{"line":20,"column":1}},"key":"FdK0PGRyLF"}],"key":"aXpEEomAtq"},{"type":"paragraph","position":{"start":{"line":22,"column":1},"end":{"line":22,"column":1}},"children":[{"type":"text","value":"This part of the architecture is totally ","position":{"start":{"line":22,"column":1},"end":{"line":22,"column":1}},"key":"R4PwAczgLq"},{"type":"strong","position":{"start":{"line":22,"column":1},"end":{"line":22,"column":1}},"children":[{"type":"text","value":"fixed","position":{"start":{"line":22,"column":1},"end":{"line":22,"column":1}},"key":"BE5u05cHOR"}],"key":"aHpoV2UrhZ"},{"type":"text","value":": the input dimension (","position":{"start":{"line":22,"column":1},"end":{"line":22,"column":1}},"key":"yA3D7hA0TO"},{"type":"inlineMath","value":"m","position":{"start":{"line":22,"column":1},"end":{"line":22,"column":1}},"html":"\u003cspan class=\"katex\"\u003e\u003cspan class=\"katex-mathml\"\u003e\u003cmath xmlns=\"http://www.w3.org/1998/Math/MathML\"\u003e\u003csemantics\u003e\u003cmrow\u003e\u003cmi\u003em\u003c/mi\u003e\u003c/mrow\u003e\u003cannotation encoding=\"application/x-tex\"\u003em\u003c/annotation\u003e\u003c/semantics\u003e\u003c/math\u003e\u003c/span\u003e\u003cspan class=\"katex-html\" aria-hidden=\"true\"\u003e\u003cspan class=\"base\"\u003e\u003cspan class=\"strut\" style=\"height:0.4306em;\"\u003e\u003c/span\u003e\u003cspan class=\"mord mathnormal\"\u003em\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e","key":"OryXmfB6Ph"},{"type":"text","value":") is determined by the quantum feature vector, and the output is always a scalar.","position":{"start":{"line":22,"column":1},"end":{"line":22,"column":1}},"key":"BIQ0fHzu18"}],"key":"sucrV7gEPp"}],"key":"K0RJjQOLsj"},{"type":"heading","depth":2,"position":{"start":{"line":26,"column":1},"end":{"line":26,"column":1}},"children":[{"type":"text","value":"Training your model","position":{"start":{"line":26,"column":1},"end":{"line":26,"column":1}},"key":"s4ewN8N7fb"}],"identifier":"sec-training","label":"sec-training","html_id":"sec-training","key":"wHNbq9j3Hd"},{"type":"paragraph","position":{"start":{"line":28,"column":1},"end":{"line":28,"column":1}},"children":[{"type":"text","value":"Once you’ve settled on your quantum layer, you can sit back, relax, and let ","position":{"start":{"line":28,"column":1},"end":{"line":28,"column":1}},"key":"BiWAM21lyp"},{"type":"inlineCode","value":"ionqvision","position":{"start":{"line":28,"column":1},"end":{"line":28,"column":1}},"key":"GoWZckccx1"},{"type":"text","value":" do the heavy lifting. First, set up your classifier and verify it’s working as expected.","position":{"start":{"line":28,"column":1},"end":{"line":28,"column":1}},"key":"u0fVDOJh9N"}],"key":"lEAvmF0LM5"}],"key":"ATVjAxLv87"},{"type":"block","kind":"notebook-code","data":{},"children":[{"type":"code","lang":"python","executable":true,"value":"from ionqvision.modules import BinaryMNISTClassifier\n\n# Set up your classifier and inspect its architecture\nclassifier = BinaryMNISTClassifier(encoder, ansatz, quantum_features); classifier","key":"SziNNM0Pgm"},{"type":"output","id":"LK95_dc27ud9VqkiRPD67","data":[{"output_type":"execute_result","execution_count":4,"metadata":{},"data":{"text/plain":{"content":"BinaryMNISTClassifier(\n  (latent_vec_encoder): Sequential(\n    (0): Linear(in_features=168, out_features=4, bias=True)\n    (1): Dropout(p=0.5, inplace=False)\n    (2): Sigmoid()\n  )\n  (quantum_layer): QuantumModule()\n  (prediction_head): Sequential(\n    (0): Linear(in_features=3, out_features=1, bias=True)\n    (1): Dropout(p=0.5, inplace=False)\n    (2): Sigmoid()\n  )\n)","content_type":"text/plain"}}}],"key":"G74x6YyJOv"}],"key":"VkQMbYZV1H"},{"type":"block","kind":"notebook-content","data":{},"children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"In case you’re wondering, ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"IKOtXZ7wHU"},{"type":"inlineCode","value":"BinaryMNISTClassifier","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"h176xoFY5j"},{"type":"text","value":" is a standard ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"u0mK3C9fGE"},{"type":"inlineCode","value":"torch.nn.Module","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Nx29AWSPmI"},{"type":"text","value":"; at this point, all the parameters in your quantum layer have been registered with ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"AyjqWNkVCg"},{"type":"inlineCode","value":"torch","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"wGSyWHv5vM"},{"type":"text","value":" and the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"j2LXyxx8Kc"},{"type":"inlineCode","value":"autograd","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Z7D4VclDzn"},{"type":"text","value":" will automatically compute the relevant gradients during the backward pass. Nothing else to worry about!","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"XqbFcMCMDq"}],"key":"P2827IS48i"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"Be sure to checkout the ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"TFA2yUC4ro"},{"type":"link","url":"https://willieab.github.io/qml-challenge-scorer/ionqvision-docs/","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"IonQ Vision Docs","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"xgfU484SJa"}],"urlSource":"https://willieab.github.io/qml-challenge-scorer/ionqvision-docs/","key":"WJOcNr2TYh"},{"type":"text","value":" to learn more about the inner workings of the ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"D5WYhhN1pW"},{"type":"inlineCode","value":"BinaryMNISTClassifier","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"qb8CDi7g0z"},{"type":"text","value":" and all the related classes. The ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"g9mNir5u1D"},{"type":"keyboard","children":[{"type":"text","value":"IonQ Vision Docs","key":"xdWApUkDqz"}],"key":"N7ZK4mxTyi"},{"type":"text","value":" button at the top-right corner of the page links directly to the API refrence.","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"urRkDm8huI"}],"key":"Qjr0lOoGI5"}],"key":"zBthROuGYh"},{"type":"block","kind":"notebook-code","data":{},"children":[{"type":"code","lang":"python","executable":true,"value":"# Check out your quantum layer\nclassifier.quantum_layer.layer_qc.draw(\"mpl\")","key":"mmhnhznMgu"},{"type":"output","id":"E3G-UWWk-pYAiI5orPoqM","data":[{"output_type":"execute_result","execution_count":5,"metadata":{},"data":{"image/png":{"content_type":"image/png","hash":"9050e03b17729dfdcbd08980b67ab512","path":"/qml-challenge-scorer/build/9050e03b17729dfdcbd08980b67ab512.png"},"text/plain":{"content":"\u003cFigure size 1625.27x367.889 with 1 Axes\u003e","content_type":"text/plain"}}}],"key":"dFdyTwwxKm"}],"key":"EQnyuLMpPL"},{"type":"block","kind":"notebook-code","data":{},"children":[{"type":"code","lang":"python","executable":true,"value":"# Verify the images loaded correctly\nclassifier.visualize_batch()","key":"BYNQpigGAB"},{"type":"output","id":"1fbIeNVuuliE5x6HCU2qF","data":[{"name":"stderr","output_type":"stream","text":"Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [0.28789353..1.9107434].\n"},{"output_type":"display_data","metadata":{},"data":{"image/png":{"content_type":"image/png","hash":"4141f44566f3144c593db2cf7f36e0c7","path":"/qml-challenge-scorer/build/4141f44566f3144c593db2cf7f36e0c7.png"},"text/plain":{"content":"\u003cFigure size 640x480 with 1 Axes\u003e","content_type":"text/plain"}}}],"key":"U5yeYvGtVK"}],"key":"KItthaMBV2"},{"type":"block","kind":"notebook-content","data":{},"children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Now train your model. Use the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"qS6B0sI4XS"},{"type":"inlineCode","value":"config","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"s2Ys9razLv"},{"type":"text","value":" dictionary to control lower-level aspects of the training, like the number of ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Y1SC4VoItn"},{"type":"inlineCode","value":"epochs","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"BBoaPRGrTl"},{"type":"text","value":", the learning rate, the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"m70DyObwLm"},{"type":"inlineCode","value":"betas","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"GtOf51kcjP"},{"type":"text","value":" used by ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"KaQ6ceQwgK"},{"type":"link","url":"https://pytorch.org/docs/stable/generated/torch.optim.Adam.html","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Adam","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"lckQCyIuJP"}],"urlSource":"https://pytorch.org/docs/stable/generated/torch.optim.Adam.html","key":"ZhOoqFDe4q"},{"type":"text","value":", etc.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"SPl8OaulSG"}],"key":"vFFCBXDjU3"}],"key":"N0HYYLpjUi"},{"type":"block","kind":"notebook-code","data":{"editable":true,"slideshow":{"slide_type":""},"tags":[]},"children":[{"type":"code","lang":"python","executable":true,"value":"# Get a (pre-processed) training and test set\ntrain_set, test_set = classifier.get_train_test_set(train_size=300, test_size=100)\n\n# Configure model training hyper parameters\nconfig = {\n    \"epochs\": 4,\n    \"lr\": 0.1,\n    \"batch_size\": 50,\n    \"betas\": (0.9, 0.99),\n    \"weight_decay\": 1e-3,\n    \"clip_grad\": True,\n    \"log_interval\": 6,\n}\n\n# Train and plot the results\nclassifier.train_module(train_set, test_set, config)\nclassifier.plot_training_progress()","visibility":"show","key":"xaj62moU5x"},{"type":"output","id":"rDurOdPAll2BIL9iNCeR0","data":[{"name":"stdout","output_type":"stream","text":"epoch:   1 | loss: 0.714\nlr: 0.1000 | processed     6/    6 batches per epoch in 140.31s (0.76s forward / 21.37s backward)\nModel achieved 54.667%  accuracy on TRAIN set.\nModel achieved 53.000%  accuracy on TEST set.\n\nepoch:   2 | loss: 0.698\nlr: 0.1000 | processed     6/    6 batches per epoch in 140.33s (0.78s forward / 21.67s backward)\nModel achieved 45.333%  accuracy on TRAIN set.\nModel achieved 47.000%  accuracy on TEST set.\n\nepoch:   3 | loss: 0.690\nlr: 0.1000 | processed     6/    6 batches per epoch in 140.98s (0.80s forward / 22.25s backward)\nModel achieved 65.000%  accuracy on TRAIN set.\nModel achieved 57.000%  accuracy on TEST set.\n\nepoch:   4 | loss: 0.695\nlr: 0.1000 | processed     6/    6 batches per epoch in 139.55s (0.74s forward / 21.01s backward)\nModel achieved 66.000%  accuracy on TRAIN set.\nModel achieved 61.000%  accuracy on TEST set.\n\n"},{"output_type":"display_data","metadata":{},"data":{"image/png":{"content_type":"image/png","hash":"f959aaa4c3821ca0035df9a894da1bdd","path":"/qml-challenge-scorer/build/f959aaa4c3821ca0035df9a894da1bdd.png"},"text/plain":{"content":"\u003cFigure size 640x480 with 1 Axes\u003e","content_type":"text/plain"}}}],"visibility":"show","key":"wSXbJPgHWP"}],"visibility":"show","key":"bMkohffFXx"},{"type":"block","kind":"notebook-content","data":{"editable":true,"slideshow":{"slide_type":""},"tags":[],"user_expressions":[{"expression":"f\"{100*classifier.train_acc[-1]:.02f}%\"","result":{"data":{"text/plain":"'66.00%'"},"metadata":{},"status":"ok"}},{"expression":"f\"{100*classifier.test_acc[-1]:.02f}%\"","result":{"data":{"text/plain":"'61.00%'"},"metadata":{},"status":"ok"}}]},"children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"The model achieved ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Bd5kvbuD8j"},{"type":"inlineExpression","value":"f\"{100*classifier.train_acc[-1]:.02f}%\"","result":{"data":{"text/plain":"'66.00%'"},"metadata":{},"status":"ok"},"identifier":"eval-1","children":[{"type":"text","value":"'66.00%'","key":"s9p7Rzz5qG"}],"key":"a68H7WkHPv"},{"type":"text","value":" on the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ZGyY2GIhd8"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"training","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"OyOp8D41ri"}],"key":"zrrK77CjhK"},{"type":"text","value":" set and ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"w5tIKJO8uK"},{"type":"inlineExpression","value":"f\"{100*classifier.test_acc[-1]:.02f}%\"","result":{"data":{"text/plain":"'61.00%'"},"metadata":{},"status":"ok"},"identifier":"eval-2","children":[{"type":"text","value":"'61.00%'","key":"TsF5jS6eGW"}],"key":"LC5ICX8qHt"},{"type":"text","value":" on the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"jHgcDQQZN9"},{"type":"strong","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"test","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"EUMv6i1WqG"}],"key":"f0OxymHfl2"},{"type":"text","value":" set.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"CC8UESEmQD"}],"key":"p0ik9tXgzu"},{"type":"heading","depth":3,"position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"Submitting your model for grading","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"lyLNSpTcP7"}],"identifier":"submitting-your-model-for-grading","label":"Submitting your model for grading","html_id":"submitting-your-model-for-grading","implicit":true,"key":"VqXoa38WMQ"},{"type":"paragraph","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"Now comes the final thrill. Use your ","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"EcHvg4oAXd"},{"type":"inlineCode","value":"classifier","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"l1B4TblyY1"},{"type":"text","value":"’s ","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"j8Od6Hdkj0"},{"type":"inlineCode","value":"submit_model_for_grading","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"nrgOX7slLP"},{"type":"text","value":" method to, well, submit your model for grading!","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"cCcQCnrk75"}],"key":"wU07eF8U8f"},{"type":"paragraph","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"text","value":"Your quantum layer and your model’s trained weights will be serialized and then reconstructed in a sanitized environment. We’ll run inference using the reconstructed model on an unseen validation set to compute your accuracy score.","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"i5jIP8uGF9"}],"key":"sUu3uicsF2"},{"type":"paragraph","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"text","value":"Be sure to check our live leader board after a few minutes to see where your team stands!","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"Lkp4R6apej"}],"key":"CX0MF0Jv5o"},{"type":"admonition","kind":"warning","children":[{"type":"admonitionTitle","children":[{"type":"text","value":"Warning","key":"bcGfp0aRYw"}],"key":"SbRWCd0QqT"},{"type":"paragraph","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"children":[{"type":"text","value":"Make sure you’ve created your team branch, e.g., using ","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"cB0ZdKGzmP"},{"type":"inlineCode","value":"git checkout -b my-team-name","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"oXKCMucjDo"},{"type":"text","value":" before running the following command.","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"e1fvBvEjyg"}],"key":"wgfDe8OOIP"},{"type":"admonition","kind":"seealso","children":[{"type":"admonitionTitle","children":[{"type":"text","value":"See Also","key":"qOY9PS7Xkz"}],"key":"m6qfAgClHM"},{"type":"paragraph","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"children":[{"type":"text","value":"Check out our ","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"key":"kOBAtXHXws"},{"type":"link","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"children":[{"type":"text","value":"setup instructions","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"key":"xD1fBUsYJb"}],"identifier":"ch-env-setup","label":"ch-env-setup","url":"/env-setup","internal":true,"dataUrl":"/env-setup.json","key":"GLCQekGvEN"},{"type":"text","value":" and our ","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"key":"lTHIKdovdS"},{"type":"link","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"children":[{"type":"text","value":"tips and tricks section","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"key":"sgphcrQYub"}],"identifier":"ch-tips-tricks","label":"ch-tips-tricks","url":"/details","internal":true,"dataUrl":"/details.json","key":"MjtX4pjc69"},{"type":"text","value":" for more ","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"key":"RXlbtBOPgS"},{"type":"crossReference","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"children":[{"type":"text","value":"details","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"key":"bKhZxsAo7z"}],"identifier":"block-team-branch","label":"block-team-branch","kind":"admonition:danger","template":"{name}","resolved":true,"html_id":"block-team-branch","remote":true,"url":"/details","dataUrl":"/details.json","key":"vKsnEn83Cc"},{"type":"text","value":".","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"key":"ilg9vSHImg"}],"key":"OBTv2QJXEw"}],"key":"H9vjjFnk9K"},{"type":"paragraph","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"children":[{"type":"text","value":"In addition, the following command assumes your current directory is the ","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"key":"s8JKNPNXQT"},{"type":"strong","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"children":[{"type":"text","value":"root","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"key":"RdgUHFw64P"}],"key":"IUdxLy6P6g"},{"type":"text","value":" of the cloned challenge repo (","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"key":"e4Y1jnw2Tg"},{"type":"inlineCode","value":"ionq-skku-vision-challenge","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"key":"LHtdp3r2eK"},{"type":"text","value":"). Check out the ","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"key":"l64mKuwadP"},{"type":"link","url":"https://willieab.github.io/qml-challenge-scorer/ionqvision-docs/modules.html#ionqvision.modules.binary_classifier.BinaryMNISTClassifier.save_model","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"children":[{"type":"text","value":"documentation","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"key":"Pjw9yKvQW7"}],"urlSource":"https://willieab.github.io/qml-challenge-scorer/ionqvision-docs/modules.html#ionqvision.modules.binary_classifier.BinaryMNISTClassifier.save_model","key":"zpTjFMMkjY"},{"type":"text","value":" of the optional ","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"key":"D5Ao55MDr9"},{"type":"inlineCode","value":"path_to_repo","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"key":"GW0i99CK0S"},{"type":"text","value":" parameter to work in a different directory.","position":{"start":{"line":19,"column":1},"end":{"line":19,"column":1}},"key":"JFEF57uOR9"}],"key":"fg7z99Luxd"}],"key":"EFtQ3ZARhT"}],"visibility":"show","key":"uAssdkCI1E"},{"type":"block","kind":"notebook-code","data":{"editable":true,"slideshow":{"slide_type":""},"tags":[]},"children":[{"type":"code","lang":"python","executable":true,"value":"classifier.submit_model_for_grading()","visibility":"show","key":"E1zPY9vuBk"},{"type":"output","id":"B2VLq5f686kMH_vO4Jukj","data":[{"name":"stdout","output_type":"stream","text":"Success! Submitted trained_models/model_e5e7ca05465cd7b5661959c722b92a7ac291834d.zip for grading!\n"}],"visibility":"show","key":"Rtp1bDt4MY"}],"visibility":"show","key":"nGI6JRvyMa"},{"type":"block","kind":"notebook-content","data":{},"children":[{"type":"admonition","kind":"note","children":[{"type":"admonitionTitle","children":[{"type":"text","value":"Note","key":"WO12KZ1O90"}],"key":"erP1JK63UA"},{"type":"paragraph","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"text","value":"The ","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"mbQECURI1p"},{"type":"inlineCode","value":"submit_model_for_grading","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"ROk1r0LNGA"},{"type":"text","value":" method prepares an archive containing your trained model’s weights, and everything that’s needed to rebuild your quantum layer. The archive is stored in the ","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"jYQtc8zLcS"},{"type":"inlineCode","value":"trained_models","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"h1RHqcBG2Q"},{"type":"text","value":" directory in your copy of our repository.","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"jbdkw6hFTu"}],"key":"rXGsPnsC0i"},{"type":"admonition","kind":"caution","children":[{"type":"admonitionTitle","children":[{"type":"text","value":"Caution","key":"xt3H4It3X9"}],"key":"uwlpXnPvG7"},{"type":"paragraph","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"text","value":"Don’t change the archive’s name or its contents.","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"tkmlxQkgII"}],"key":"K1MgjyODRs"}],"key":"zcmKs0IjRE"}],"key":"Y96ewX1Zb5"},{"type":"paragraph","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"text","value":"Good luck now! We hope you enjoy this exciting challenge.","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"YE2z0ySNbG"}],"key":"HdsUDjHhT1"}],"key":"nZ1i0TA4iZ"}],"key":"gLS0v0lb7c"},"references":{"cite":{"order":[],"data":{}}},"footer":{"navigation":{"prev":{"title":"Zero or one?","url":"/","group":"Zero or one?"},"next":{"title":"Tips, tricks, and potential gotchas","url":"/details","group":"Zero or one?"}}},"domain":"http://localhost:3000"},"project":{"subject":"Quantum Vision Challenge","exports":[],"title":"Zero or one?","description":"An image classification challenge using hybrid quantum-classical neural networks","banner":"/qml-challenge-scorer/build/forte_header-caf18de3d14967ab2dea6f0808cc24cd.png","authors":[{"id":"Willie Aboumrad","name":"Willie Aboumrad"},{"id":"Sang Kim","name":"Sang Kim"}],"venue":{"title":"IonQ-SKKU Hackathon"},"github":"https://github.com/willieab/ionq-skku-vision-challenge","copyright":"IonQ, Inc (2024)","bibliography":[],"index":"index","pages":[{"slug":"challenge-description","title":"Your mission","description":"","date":"","thumbnail":"/qml-challenge-scorer/build/hybrid_nn-02bee137d9c820208ac883f635840f61.png","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"details","title":"Tips, tricks, and potential gotchas","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"env-setup","title":"Environment set up","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"leaderboard","title":"Leader board","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1}]}}},"actionData":null,"errors":null},"future":{"unstable_dev":false,"unstable_postcss":false,"unstable_tailwind":false,"v2_errorBoundary":false,"v2_headers":true,"v2_meta":true,"v2_normalizeFormMethod":true,"v2_routeConvention":true}};</script><script type="module" async="">import "/qml-challenge-scorer/build/manifest-B2CC8BBF.js";
import * as route0 from "/qml-challenge-scorer/build/root-3WANWRXC.js";
import * as route1 from "/qml-challenge-scorer/build/routes/$-CZ6MXPDJ.js";
window.__remixRouteModules = {"root":route0,"routes/$":route1};

import("/qml-challenge-scorer/build/entry.client-K6EYMIDZ.js");</script></body></html>